# =============================================================================
# ZEROPOWER BACKEND SELECTION - CHANGE THIS LINE TO SWITCH METHODS
# =============================================================================
ZEROPOWER_METHOD = "svd_polar"  # Options: "newton_schulz", "svd_polar", "tanh_element", "tanh_matrix"

import sys
with open(sys.argv[0]) as f:
    code = f.read()

import os
import uuid
import time
import copy
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path
import itertools

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True
# torch._dynamo.config.compiled_autograd = True  # Disabled due to FlexAttention incompatibility

# =============================================================================
# ZEROPOWER BACKEND IMPLEMENTATIONS
# =============================================================================

def zeropower_via_newtonschulz5(G: Tensor) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G.
    """
    assert G.ndim >= 2
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for a, b, c in [
        (4.0848, -6.8946, 2.9270),
        (3.9505, -6.3029, 2.6377),
        (3.7418, -5.5913, 2.3037),
        (2.8769, -3.1427, 1.2046),
        (2.8366, -3.0525, 1.2012),
    ]:
        A = X @ X.mT
        B = b * A + c * A @ A
        X = a * X + B @ X

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

def zeropower_via_svd_polar(G: Tensor) -> Tensor:
    """
    SVD-based polar decomposition for orthogonalization.
    """
    try:
        # Handle non-square matrices like Newton-Schulz does
        was_transposed = False
        X = G.float()
        if G.size(-2) > G.size(-1):
            X = X.mT
            was_transposed = True
        
        # Use modern linalg.svd instead of deprecated torch.svd
        U, S, Vh = torch.linalg.svd(X, full_matrices=False)
        # Note: torch.linalg.svd returns Vh (V hermitian), not V
        result = U @ Vh
        
        if was_transposed:
            result = result.mT
            
        return result.to(G.dtype)
    except Exception:
        # Fallback to Newton-Schulz if SVD fails
        return zeropower_via_newtonschulz5(G)

def zeropower_via_tanh_element(G: Tensor, alpha: float = 100.0) -> Tensor:
    """
    Element-wise tanh approximation: tanh(alpha * G) / tanh(alpha).
    """
    tanh_alpha = torch.tanh(torch.tensor(alpha, dtype=G.dtype, device=G.device))
    return torch.tanh(alpha * G) / tanh_alpha

def zeropower_via_tanh_matrix(G: Tensor, alpha: float = 10.0) -> Tensor:
    """
    Matrix tanh approximation (simplified version to avoid hangs).
    Falls back to element-wise for safety.
    """
    # For now, use element-wise tanh to avoid the hanging issues we discovered
    # This can be improved with a proper safe matrix tanh implementation
    return zeropower_via_tanh_element(G, alpha)

# =============================================================================
# ZEROPOWER BACKEND REGISTRY AND SELECTOR
# =============================================================================

ZEROPOWER_BACKENDS = {
    "newton_schulz": zeropower_via_newtonschulz5,
    "svd_polar": zeropower_via_svd_polar,
    "tanh_element": zeropower_via_tanh_element,
    "tanh_matrix": zeropower_via_tanh_matrix,
}

def get_zeropower_function():
    """Get the currently selected zeropower function."""
    if ZEROPOWER_METHOD not in ZEROPOWER_BACKENDS:
        available = ", ".join(ZEROPOWER_BACKENDS.keys())
        raise ValueError(f"Unknown zeropower method '{ZEROPOWER_METHOD}'. Available: {available}")
    return ZEROPOWER_BACKENDS[ZEROPOWER_METHOD]

# Set the global zeropower function
zeropower_func = get_zeropower_function()

# =============================================================================
# CONFIGURABLE MUON OPTIMIZER
# =============================================================================

@torch.compile
def update(acc_bf16_view_u16: Tensor, mantissa: Tensor, momentum_buffer: Tensor, grad: Tensor, momentum: Tensor, eff_lr: Tensor, eff_weight_decay: Tensor):
    assert acc_bf16_view_u16.dtype == mantissa.dtype == torch.uint16
    grad = grad.float()
    momentum_buffer.copy_(momentum * momentum_buffer + (1 - momentum) * grad)
    v = zeropower_func(momentum * momentum_buffer + (1 - momentum) * grad)

    acc_m_u32 = (acc_bf16_view_u16.to(torch.uint32) << 16) | mantissa.to(torch.uint32)
    acc_m_u32.view(torch.float32).mul_(1 - eff_weight_decay)
    acc_m_u32.view(torch.float32).add_(other=v, alpha=-eff_lr)
    acc_bf16_view_u16.copy_((acc_m_u32 >> 16).to(torch.uint16))
    mantissa.copy_(acc_m_u32.to(torch.uint16))

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by configurable zeropower method.
    
    This version allows switching between different orthogonalization backends
    by changing the ZEROPOWER_METHOD variable at the top of this file.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        super().__init__(params, defaults)
        assert all(p.dtype == torch.bfloat16 for group in self.param_groups for p in group["params"])

    @torch.no_grad()
    def step(self):
        futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * self.world_size
            momentum = torch._as_tensor_fullprec(group["momentum"])
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    state = self.state[p]
                    if len(state) == 0:
                        state["mantissa"] = torch.zeros_like(p, dtype=torch.uint16)
                        state["momentum_buffer"] = torch.zeros_like(p, dtype=torch.float32)
                    update(
                        p.view(torch.uint16), state["mantissa"], state["momentum_buffer"],
                        p.grad, momentum,
                        eff_lr=torch._as_tensor_fullprec(group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5),
                        eff_weight_decay=torch._as_tensor_fullprec(group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)),
                    )
                futures.append(dist.all_gather(params_pad[base_i:base_i + self.world_size], params_pad[base_i + self.rank], async_op=True).get_future())
        torch.futures.collect_all(futures).wait()

# =============================================================================
# MODEL COMPONENTS (copied from utils.py)
# =============================================================================

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

@torch.no_grad()
def init_linear(w: Tensor):
    std = 0.5 * (w.size(-1) ** -0.5)
    bound = (3 ** 0.5) * std
    return w.uniform_(-bound, bound)

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        self.qkvo_w = nn.Parameter(init_linear(torch.empty(4, hdim, dim)).bfloat16())
        self.qkvo_w.detach()[3].zero_()
        self.rotary = Rotary(head_dim, max_seq_len)
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask, lambdas: Tensor):
        B, T = x.size(0), x.size(1)
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkvo_w[:3].flatten(end_dim=1)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k)
        q, k = self.rotary(q), self.rotary(k)
        v = norm(v)
        if ve is not None:
            v = lambdas[0] * v + lambdas[1] * ve.view_as(v)
        else:
            v = lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim)
        y = F.linear(y, self.qkvo_w[3])
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.fc_w = nn.Parameter(init_linear(torch.empty(hdim, dim)).bfloat16())
        self.proj_w = nn.Parameter(torch.zeros(dim, hdim).bfloat16())
        self.fc_w.wd_mul = 2.0
        self.proj_w.wd_mul = 2.0

    def forward(self, x: Tensor):
        x = F.linear(x, self.fc_w)
        x = F.relu(x).square()
        x = F.linear(x, self.proj_w)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, layer_idx: int):
        super().__init__()
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask, lambdas: Tensor, sa_lambdas: Tensor):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(x, ve, block_mask, sa_lambdas)
        x = x + self.mlp(norm(x))
        return x

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, max_seq_len, i) for i in range(num_layers)])
        self.lm_head_w = nn.Parameter(torch.zeros(next_multiple_of_n(vocab_size, n=128), model_dim))
        assert num_layers % 2 == 0
        self.scalars = nn.Parameter(torch.cat([
            torch.ones(num_layers),
            *[torch.tensor([1.0, 0.0]) for _ in range(num_layers)],
            *[torch.tensor([0.5, 0.5]) for _ in range(num_layers)],
        ]))

    def create_blockmasks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_blockmask: Tensor):
            num_blocks = dense_blockmask.sum(dim=-1, dtype=torch.int32)
            indices = dense_blockmask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        causal_blockmask_any = block_idx[:, None] >= block_idx
        causal_blockmask_all = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        document_blockmask_any = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        document_blockmask_all = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        blockmask_any = causal_blockmask_any & document_blockmask_any
        blockmask_all = causal_blockmask_all & document_blockmask_all
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(blockmask_any & ~blockmask_all)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(blockmask_all)
        def build_bm(window_size_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(window_size_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, window_size_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = self.create_blockmasks(input_seq, sliding_window_num_blocks)
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, long_bm]
        assert len(block_masks) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None])

        skip_connections = []
        skip_map = {9: 6, 10: 4, 11: 2}
        skip_weights = self.scalars[:len(self.blocks)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        for i in range(len(self.blocks)):
            if i in skip_map:
                x = x + skip_weights[skip_map[i]] * skip_connections[skip_map[i]]
            x = self.blocks[i](x, ve[i], x0, block_masks[i], lambdas[i], sa_lambdas[i])
            skip_connections.append(x)

        x = norm(x)
        if self.training:
            logits: Tensor = F.linear(x.flatten(end_dim=1), self.lm_head_w.bfloat16()).float()
            loss = F.cross_entropy(15 * logits * torch.rsqrt(logits.square() + 225), target_seq)
            return loss

        loss = 0
        for i in range(4):
            logits: Tensor = F.linear(x.flatten(end_dim=1).chunk(4)[i], self.lm_head_w.bfloat16()).float()
            loss += F.cross_entropy(15 * logits * torch.rsqrt(logits.square() + 225), target_seq.chunk(4)[i]) / 4
        return loss

# =============================================================================
# DATA LOADER
# =============================================================================

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32)
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2])
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True)
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy())
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files)
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True)
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True)
        pos += batch_size
        yield inputs, targets

# =============================================================================
# UTILITY FUNCTIONS
# =============================================================================

@dataclass
class Hyperparameters:
    train_files = "data/fineweb10B/fineweb_train_*.bin"
    val_files = "data/fineweb10B/fineweb_val_*.bin"
    val_tokens = 10485760
    train_seq_len = 64*1024
    val_seq_len = 4*64*1024
    num_iterations = 5960
    cooldown_frac = 0.7
    vocab_size = 50257
    val_loss_every = 125
    save_checkpoint = False

def nvidia_smi():
    import subprocess
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout

def get_lr(step: int, num_iterations: int, cooldown_frac: float):
    x = step / num_iterations
    assert 0 <= x < 1
    if x < 1 - cooldown_frac:
        return 1.0
    else:
        return (1 - x) / cooldown_frac

@lru_cache(1)
def get_window_size_blocks_helper(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

def get_window_size_blocks(step: int, num_iterations: int):
    x = step / num_iterations
    assert 0 <= x <= 1
    factor = 4 * x ** 3 - 6 * x ** 2 + 3 * x
    window_size = next_multiple_of_n(3456 * factor, n=128)
    return get_window_size_blocks_helper(window_size)

def setup_distributed_training():
    run_id = int(os.environ.get("RUN_ID", 0))
    rank = int(os.environ["RANK"])
    world_size = int(os.environ["WORLD_SIZE"])
    assert world_size == 8
    assert torch.cuda.is_available()
    device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
    torch.cuda.set_device(device)
    dist.init_process_group(backend="nccl", device_id=device)
    dist.barrier()
    master_process = (rank == 0)
    return run_id, rank, world_size, device, master_process

def setup_logging(run_id, master_process):
    if master_process:
        run_id_full = f"{run_id:03d}_{uuid.uuid4()}"
        os.makedirs("logs", exist_ok=True)
        logfile = f"logs/{run_id_full}.txt"
        print(logfile)
    else:
        run_id_full = None
        logfile = None
    
    def print0(s, console=False):
        if master_process:
            with open(logfile, "a") as f:
                if console:
                    print(s)
                print(s, file=f)
    
    return print0, run_id_full, logfile

def log_system_info(print0, code):
    print0(code)
    print0("="*100)
    print0(f"Running Python {sys.version}")
    print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
    print0(f"ZEROPOWER METHOD: {ZEROPOWER_METHOD}")
    print0(nvidia_smi())
    print0("="*100)

def create_model_and_optimizers(args, rank, world_size):
    model: nn.Module = GPT(vocab_size=args.vocab_size, num_layers=16, num_heads=8, model_dim=1024,
                           max_seq_len=max(args.train_seq_len, args.val_seq_len)).cuda()
    for m in model.modules():
        if isinstance(m, nn.Embedding):
            m.bfloat16()
    for param in model.parameters():
        dist.broadcast(param.detach(), 0)

    embed_params = [*model.embed.parameters(), *model.value_embeds.parameters()]
    scalar_params = [model.scalars]
    head_params: list[nn.Parameter] = [model.lm_head_w]
    hidden_matrix_params = sorted((p for p in model.blocks.parameters() if p.ndim >= 2), key=lambda x: x.size(), reverse=True)

    adam_param_groups = [
        dict(params=head_params, lr=1/320), 
        dict(params=embed_params, lr=0.3), 
        dict(params=scalar_params, lr=0.015)
    ]
    optimizer1 = torch.optim.AdamW(adam_param_groups, betas=(0.8, 0.95), eps=1e-10, weight_decay=0.0, fused=True)
    optimizer2 = Muon(hidden_matrix_params, lr=0.025, momentum=0.95, rank=rank, world_size=world_size)
    optimizers = [optimizer1, optimizer2]

    opt2params = {opt: [p for group in opt.param_groups for p in group["params"]] for opt in optimizers}
    for opt in optimizers:
        for group in opt.param_groups:
            group["initial_lr"] = group["lr"]

    return model, optimizers, opt2params

def warmup_kernels(model, optimizers, args):
    warmup_steps = 10
    initial_state = copy.deepcopy(dict(model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers]))
    for _ in range(warmup_steps):
        inputs = targets = torch.randint(0, args.vocab_size, size=(args.train_seq_len,), device="cuda")
        model(inputs.to(torch.int32), targets, get_window_size_blocks(0, args.num_iterations)).backward()
        for param in model.parameters():
            dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
        for opt in optimizers:
            opt.step()
        model.zero_grad(set_to_none=True)
    model.load_state_dict(initial_state["model"])
    for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
        opt.load_state_dict(opt_state)
    del initial_state

# =============================================================================
# MAIN TRAINING LOOP
# =============================================================================

# Setup
run_id, rank, world_size, device, master_process = setup_distributed_training()
print0, run_id_full, logfile = setup_logging(run_id, master_process)
log_system_info(print0, code)

args = Hyperparameters()
model, optimizers, opt2params = create_model_and_optimizers(args, rank, world_size)
model = torch.compile(model, dynamic=False)
warmup_kernels(model, optimizers, args)

torch.cuda.reset_peak_memory_stats()
train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, rank, world_size)
training_time_ms = 0
dist.barrier()
t0 = time.perf_counter()

for step in range(args.num_iterations + 1):
    last_step = (step == args.num_iterations)

    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        dist.barrier()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size, rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets = next(val_loader)
                val_loss += model(inputs, targets, get_window_size_blocks(step, args.num_iterations))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{args.num_iterations} val_loss:{val_loss:.6f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        dist.barrier()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id_full}", exist_ok=True)
            torch.save(log, f"logs/{run_id_full}/state_step{step:06d}.pt")
        break

    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(step, args.num_iterations)).backward()
    opt2futures = {
        opt: [dist.all_reduce(p.grad, op=dist.ReduceOp.AVG, async_op=True).get_future() for p in params]
        for opt, params in opt2params.items()
    }
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step, args.num_iterations, args.cooldown_frac)
    if len(optimizers) > 1:  # Handle Muon momentum warmup
        for group in optimizers[1].param_groups:
            frac = min(step / 300, 1)
            group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    for opt in optimizers:
        torch.futures.collect_all(opt2futures[opt]).wait()
        opt.step()
    model.zero_grad(set_to_none=True)
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{args.num_iterations} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group() 
====================================================================================================
Running Python 3.12.11 | packaged by Anaconda, Inc. | (main, Jun  5 2025, 13:09:17) [GCC 11.2.0]
Running PyTorch 2.7.1+cu126 compiled for CUDA 12.6
ZEROPOWER METHOD: svd_polar
Sun Jul  6 08:08:56 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   25C    P0            108W /  700W |    5842MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   23C    P0            110W /  700W |    1504MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   23C    P0            109W /  700W |    1504MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   23C    P0            106W /  700W |    1504MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   25C    P0            111W /  700W |    1504MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   24C    P0            108W /  700W |    1504MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   24C    P0            112W /  700W |    1504MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   23C    P0            108W /  700W |    1504MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/5960 val_loss:10.825837 train_time:0ms step_avg:0.31ms
step:1/5960 train_time:299ms step_avg:299.25ms
step:2/5960 train_time:1240ms step_avg:619.79ms
step:3/5960 train_time:2201ms step_avg:733.78ms
step:4/5960 train_time:3163ms step_avg:790.73ms
step:5/5960 train_time:4133ms step_avg:826.59ms
step:6/5960 train_time:5098ms step_avg:849.70ms
step:7/5960 train_time:6062ms step_avg:865.94ms
step:8/5960 train_time:7034ms step_avg:879.25ms
step:9/5960 train_time:8017ms step_avg:890.76ms
step:10/5960 train_time:8990ms step_avg:899.04ms
step:11/5960 train_time:9971ms step_avg:906.50ms
step:12/5960 train_time:10943ms step_avg:911.95ms
step:13/5960 train_time:11923ms step_avg:917.18ms
step:14/5960 train_time:12906ms step_avg:921.85ms
step:15/5960 train_time:13885ms step_avg:925.67ms
step:16/5960 train_time:14872ms step_avg:929.51ms
step:17/5960 train_time:15845ms step_avg:932.07ms
step:18/5960 train_time:16832ms step_avg:935.10ms
step:19/5960 train_time:17811ms step_avg:937.42ms
step:20/5960 train_time:18796ms step_avg:939.78ms
step:21/5960 train_time:19776ms step_avg:941.73ms
step:22/5960 train_time:20763ms step_avg:943.77ms
step:23/5960 train_time:21751ms step_avg:945.68ms
step:24/5960 train_time:22729ms step_avg:947.04ms
step:25/5960 train_time:23716ms step_avg:948.65ms
step:26/5960 train_time:24699ms step_avg:949.96ms
step:27/5960 train_time:25683ms step_avg:951.23ms
step:28/5960 train_time:26653ms step_avg:951.88ms
step:29/5960 train_time:27634ms step_avg:952.89ms
step:30/5960 train_time:28617ms step_avg:953.89ms
step:31/5960 train_time:29599ms step_avg:954.79ms
step:32/5960 train_time:30574ms step_avg:955.43ms
step:33/5960 train_time:31546ms step_avg:955.93ms
step:34/5960 train_time:32524ms step_avg:956.60ms
step:35/5960 train_time:33500ms step_avg:957.15ms
step:36/5960 train_time:34478ms step_avg:957.74ms
step:37/5960 train_time:35451ms step_avg:958.14ms
step:38/5960 train_time:36430ms step_avg:958.69ms
step:39/5960 train_time:37408ms step_avg:959.18ms
step:40/5960 train_time:38384ms step_avg:959.60ms
step:41/5960 train_time:39360ms step_avg:959.99ms
step:42/5960 train_time:40341ms step_avg:960.50ms
step:43/5960 train_time:41309ms step_avg:960.67ms
step:44/5960 train_time:42279ms step_avg:960.88ms
step:45/5960 train_time:43271ms step_avg:961.59ms
step:46/5960 train_time:44230ms step_avg:961.52ms
step:47/5960 train_time:45214ms step_avg:962.01ms
step:48/5960 train_time:46188ms step_avg:962.25ms
step:49/5960 train_time:47155ms step_avg:962.35ms
step:50/5960 train_time:48134ms step_avg:962.68ms
step:51/5960 train_time:49109ms step_avg:962.92ms
step:52/5960 train_time:50087ms step_avg:963.21ms
step:53/5960 train_time:51055ms step_avg:963.30ms
step:54/5960 train_time:52029ms step_avg:963.50ms
step:55/5960 train_time:52998ms step_avg:963.60ms
step:56/5960 train_time:53974ms step_avg:963.83ms
step:57/5960 train_time:54950ms step_avg:964.04ms
step:58/5960 train_time:55919ms step_avg:964.11ms
step:59/5960 train_time:56886ms step_avg:964.17ms
step:60/5960 train_time:57860ms step_avg:964.33ms
step:61/5960 train_time:58824ms step_avg:964.33ms
step:62/5960 train_time:59802ms step_avg:964.54ms
step:63/5960 train_time:60775ms step_avg:964.68ms
step:64/5960 train_time:61741ms step_avg:964.70ms
step:65/5960 train_time:62714ms step_avg:964.83ms
step:66/5960 train_time:63689ms step_avg:964.98ms
step:67/5960 train_time:64662ms step_avg:965.10ms
step:68/5960 train_time:65626ms step_avg:965.08ms
step:69/5960 train_time:66597ms step_avg:965.17ms
step:70/5960 train_time:67571ms step_avg:965.30ms
step:71/5960 train_time:68543ms step_avg:965.39ms
step:72/5960 train_time:69514ms step_avg:965.47ms
step:73/5960 train_time:70476ms step_avg:965.42ms
step:74/5960 train_time:71445ms step_avg:965.47ms
step:75/5960 train_time:72411ms step_avg:965.47ms
step:76/5960 train_time:73375ms step_avg:965.45ms
step:77/5960 train_time:74351ms step_avg:965.60ms
step:78/5960 train_time:75324ms step_avg:965.69ms
step:79/5960 train_time:76298ms step_avg:965.79ms
step:80/5960 train_time:77265ms step_avg:965.81ms
step:81/5960 train_time:78232ms step_avg:965.83ms
step:82/5960 train_time:79209ms step_avg:965.97ms
step:83/5960 train_time:80169ms step_avg:965.89ms
step:84/5960 train_time:81140ms step_avg:965.96ms
step:85/5960 train_time:82114ms step_avg:966.05ms
step:86/5960 train_time:83072ms step_avg:965.95ms
step:87/5960 train_time:84040ms step_avg:965.97ms
step:88/5960 train_time:85012ms step_avg:966.05ms
step:89/5960 train_time:85973ms step_avg:965.99ms
step:90/5960 train_time:86949ms step_avg:966.10ms
step:91/5960 train_time:87913ms step_avg:966.07ms
step:92/5960 train_time:88882ms step_avg:966.11ms
step:93/5960 train_time:89845ms step_avg:966.07ms
step:94/5960 train_time:90817ms step_avg:966.14ms
step:95/5960 train_time:91780ms step_avg:966.10ms
step:96/5960 train_time:92746ms step_avg:966.11ms
step:97/5960 train_time:93712ms step_avg:966.10ms
step:98/5960 train_time:94677ms step_avg:966.10ms
step:99/5960 train_time:95644ms step_avg:966.10ms
step:100/5960 train_time:96616ms step_avg:966.16ms
step:101/5960 train_time:97577ms step_avg:966.11ms
step:102/5960 train_time:98533ms step_avg:966.01ms
step:103/5960 train_time:99505ms step_avg:966.07ms
step:104/5960 train_time:100465ms step_avg:966.01ms
step:105/5960 train_time:101428ms step_avg:965.98ms
step:106/5960 train_time:102395ms step_avg:965.99ms
step:107/5960 train_time:103347ms step_avg:965.86ms
step:108/5960 train_time:104312ms step_avg:965.85ms
step:109/5960 train_time:105275ms step_avg:965.83ms
step:110/5960 train_time:106235ms step_avg:965.77ms
step:111/5960 train_time:107214ms step_avg:965.89ms
step:112/5960 train_time:108172ms step_avg:965.82ms
step:113/5960 train_time:109123ms step_avg:965.69ms
step:114/5960 train_time:110094ms step_avg:965.73ms
step:115/5960 train_time:111054ms step_avg:965.69ms
step:116/5960 train_time:112015ms step_avg:965.64ms
step:117/5960 train_time:112978ms step_avg:965.62ms
step:118/5960 train_time:113940ms step_avg:965.60ms
step:119/5960 train_time:114901ms step_avg:965.56ms
step:120/5960 train_time:115864ms step_avg:965.53ms
step:121/5960 train_time:116819ms step_avg:965.45ms
step:122/5960 train_time:117794ms step_avg:965.52ms
step:123/5960 train_time:118747ms step_avg:965.43ms
step:124/5960 train_time:119709ms step_avg:965.40ms
step:125/5960 train_time:120666ms step_avg:965.33ms
step:125/5960 val_loss:4.306984 train_time:120677ms step_avg:965.42ms
step:126/5960 train_time:121639ms step_avg:965.39ms
step:127/5960 train_time:122589ms step_avg:965.27ms
step:128/5960 train_time:123555ms step_avg:965.27ms
step:129/5960 train_time:124503ms step_avg:965.14ms
step:130/5960 train_time:125485ms step_avg:965.27ms
step:131/5960 train_time:126445ms step_avg:965.23ms
step:132/5960 train_time:127398ms step_avg:965.14ms
step:133/5960 train_time:128363ms step_avg:965.13ms
step:134/5960 train_time:129333ms step_avg:965.17ms
step:135/5960 train_time:130286ms step_avg:965.08ms
step:136/5960 train_time:131249ms step_avg:965.07ms
step:137/5960 train_time:132210ms step_avg:965.04ms
step:138/5960 train_time:133179ms step_avg:965.07ms
step:139/5960 train_time:134133ms step_avg:964.99ms
step:140/5960 train_time:135088ms step_avg:964.92ms
step:141/5960 train_time:136051ms step_avg:964.90ms
step:142/5960 train_time:137010ms step_avg:964.86ms
step:143/5960 train_time:137964ms step_avg:964.78ms
step:144/5960 train_time:138928ms step_avg:964.78ms
step:145/5960 train_time:139886ms step_avg:964.73ms
step:146/5960 train_time:140846ms step_avg:964.70ms
step:147/5960 train_time:141794ms step_avg:964.59ms
step:148/5960 train_time:142766ms step_avg:964.64ms
step:149/5960 train_time:143723ms step_avg:964.59ms
step:150/5960 train_time:144682ms step_avg:964.54ms
step:151/5960 train_time:145643ms step_avg:964.53ms
step:152/5960 train_time:146592ms step_avg:964.42ms
step:153/5960 train_time:147563ms step_avg:964.46ms
step:154/5960 train_time:148509ms step_avg:964.34ms
step:155/5960 train_time:149472ms step_avg:964.34ms
step:156/5960 train_time:150428ms step_avg:964.28ms
step:157/5960 train_time:151391ms step_avg:964.27ms
step:158/5960 train_time:152338ms step_avg:964.16ms
step:159/5960 train_time:153301ms step_avg:964.15ms
step:160/5960 train_time:154256ms step_avg:964.10ms
step:161/5960 train_time:155219ms step_avg:964.09ms
step:162/5960 train_time:156176ms step_avg:964.05ms
step:163/5960 train_time:157134ms step_avg:964.01ms
step:164/5960 train_time:158085ms step_avg:963.93ms
step:165/5960 train_time:159052ms step_avg:963.95ms
step:166/5960 train_time:160013ms step_avg:963.94ms
step:167/5960 train_time:160966ms step_avg:963.87ms
step:168/5960 train_time:161921ms step_avg:963.81ms
step:169/5960 train_time:162877ms step_avg:963.77ms
step:170/5960 train_time:163844ms step_avg:963.79ms
step:171/5960 train_time:164797ms step_avg:963.73ms
step:172/5960 train_time:165763ms step_avg:963.74ms
step:173/5960 train_time:166718ms step_avg:963.69ms
step:174/5960 train_time:167672ms step_avg:963.63ms
step:175/5960 train_time:168637ms step_avg:963.64ms
step:176/5960 train_time:169596ms step_avg:963.61ms
step:177/5960 train_time:170557ms step_avg:963.60ms
step:178/5960 train_time:171518ms step_avg:963.58ms
step:179/5960 train_time:172484ms step_avg:963.60ms
step:180/5960 train_time:173430ms step_avg:963.50ms
step:181/5960 train_time:174385ms step_avg:963.45ms
step:182/5960 train_time:175345ms step_avg:963.44ms
step:183/5960 train_time:176311ms step_avg:963.45ms
step:184/5960 train_time:177259ms step_avg:963.36ms
step:185/5960 train_time:178216ms step_avg:963.33ms
step:186/5960 train_time:179176ms step_avg:963.31ms
step:187/5960 train_time:180129ms step_avg:963.26ms
step:188/5960 train_time:181083ms step_avg:963.21ms
step:189/5960 train_time:182048ms step_avg:963.22ms
step:190/5960 train_time:183002ms step_avg:963.17ms
step:191/5960 train_time:183985ms step_avg:963.27ms
step:192/5960 train_time:184944ms step_avg:963.25ms
step:193/5960 train_time:185905ms step_avg:963.24ms
step:194/5960 train_time:186858ms step_avg:963.19ms
step:195/5960 train_time:187814ms step_avg:963.15ms
step:196/5960 train_time:188769ms step_avg:963.11ms
step:197/5960 train_time:189733ms step_avg:963.11ms
step:198/5960 train_time:190679ms step_avg:963.03ms
step:199/5960 train_time:191631ms step_avg:962.97ms
step:200/5960 train_time:192588ms step_avg:962.94ms
step:201/5960 train_time:193541ms step_avg:962.89ms
step:202/5960 train_time:194490ms step_avg:962.82ms
step:203/5960 train_time:195450ms step_avg:962.81ms
step:204/5960 train_time:196418ms step_avg:962.83ms
step:205/5960 train_time:197370ms step_avg:962.78ms
step:206/5960 train_time:198330ms step_avg:962.77ms
step:207/5960 train_time:199281ms step_avg:962.71ms
step:208/5960 train_time:200248ms step_avg:962.73ms
step:209/5960 train_time:201207ms step_avg:962.71ms
step:210/5960 train_time:202159ms step_avg:962.66ms
step:211/5960 train_time:203114ms step_avg:962.62ms
step:212/5960 train_time:204066ms step_avg:962.57ms
step:213/5960 train_time:205023ms step_avg:962.55ms
step:214/5960 train_time:205979ms step_avg:962.52ms
step:215/5960 train_time:206928ms step_avg:962.45ms
step:216/5960 train_time:207877ms step_avg:962.40ms
step:217/5960 train_time:208843ms step_avg:962.41ms
step:218/5960 train_time:209785ms step_avg:962.32ms
step:219/5960 train_time:210757ms step_avg:962.36ms
step:220/5960 train_time:211707ms step_avg:962.31ms
step:221/5960 train_time:212663ms step_avg:962.28ms
step:222/5960 train_time:213616ms step_avg:962.24ms
step:223/5960 train_time:214571ms step_avg:962.20ms
step:224/5960 train_time:215520ms step_avg:962.14ms
step:225/5960 train_time:216479ms step_avg:962.13ms
step:226/5960 train_time:217434ms step_avg:962.10ms
step:227/5960 train_time:218377ms step_avg:962.01ms
step:228/5960 train_time:219335ms step_avg:962.00ms
step:229/5960 train_time:220292ms step_avg:961.98ms
step:230/5960 train_time:221242ms step_avg:961.92ms
step:231/5960 train_time:222209ms step_avg:961.94ms
step:232/5960 train_time:223159ms step_avg:961.89ms
step:233/5960 train_time:224118ms step_avg:961.88ms
step:234/5960 train_time:225073ms step_avg:961.85ms
step:235/5960 train_time:226023ms step_avg:961.80ms
step:236/5960 train_time:226976ms step_avg:961.76ms
step:237/5960 train_time:227928ms step_avg:961.72ms
step:238/5960 train_time:228878ms step_avg:961.67ms
step:239/5960 train_time:229829ms step_avg:961.63ms
step:240/5960 train_time:230778ms step_avg:961.58ms
step:241/5960 train_time:231743ms step_avg:961.59ms
step:242/5960 train_time:232712ms step_avg:961.62ms
step:243/5960 train_time:233662ms step_avg:961.57ms
step:244/5960 train_time:234628ms step_avg:961.59ms
step:245/5960 train_time:235588ms step_avg:961.58ms
step:246/5960 train_time:236549ms step_avg:961.58ms
step:247/5960 train_time:237515ms step_avg:961.60ms
step:248/5960 train_time:238464ms step_avg:961.55ms
step:249/5960 train_time:239425ms step_avg:961.55ms
step:250/5960 train_time:240384ms step_avg:961.53ms
step:250/5960 val_loss:3.867513 train_time:240393ms step_avg:961.57ms
step:251/5960 train_time:241348ms step_avg:961.55ms
step:252/5960 train_time:242307ms step_avg:961.53ms
step:253/5960 train_time:243264ms step_avg:961.52ms
step:254/5960 train_time:244226ms step_avg:961.52ms
step:255/5960 train_time:245181ms step_avg:961.49ms
step:256/5960 train_time:246137ms step_avg:961.47ms
step:257/5960 train_time:247095ms step_avg:961.46ms
step:258/5960 train_time:248055ms step_avg:961.45ms
step:259/5960 train_time:249007ms step_avg:961.42ms
step:260/5960 train_time:249967ms step_avg:961.41ms
step:261/5960 train_time:250924ms step_avg:961.39ms
step:262/5960 train_time:251884ms step_avg:961.39ms
step:263/5960 train_time:252836ms step_avg:961.35ms
step:264/5960 train_time:253800ms step_avg:961.36ms
step:265/5960 train_time:254757ms step_avg:961.35ms
step:266/5960 train_time:255720ms step_avg:961.35ms
step:267/5960 train_time:256669ms step_avg:961.31ms
step:268/5960 train_time:257628ms step_avg:961.30ms
step:269/5960 train_time:258591ms step_avg:961.30ms
step:270/5960 train_time:259547ms step_avg:961.29ms
step:271/5960 train_time:260497ms step_avg:961.25ms
step:272/5960 train_time:261465ms step_avg:961.27ms
step:273/5960 train_time:262419ms step_avg:961.24ms
step:274/5960 train_time:263382ms step_avg:961.25ms
step:275/5960 train_time:264344ms step_avg:961.25ms
step:276/5960 train_time:265299ms step_avg:961.23ms
step:277/5960 train_time:266254ms step_avg:961.21ms
step:278/5960 train_time:267216ms step_avg:961.21ms
step:279/5960 train_time:268181ms step_avg:961.22ms
step:280/5960 train_time:269128ms step_avg:961.17ms
step:281/5960 train_time:270089ms step_avg:961.17ms
step:282/5960 train_time:271047ms step_avg:961.16ms
step:283/5960 train_time:272010ms step_avg:961.17ms
step:284/5960 train_time:272963ms step_avg:961.14ms
step:285/5960 train_time:273917ms step_avg:961.11ms
step:286/5960 train_time:274876ms step_avg:961.11ms
step:287/5960 train_time:275838ms step_avg:961.11ms
step:288/5960 train_time:276799ms step_avg:961.11ms
step:289/5960 train_time:277751ms step_avg:961.08ms
step:290/5960 train_time:278713ms step_avg:961.08ms
step:291/5960 train_time:279670ms step_avg:961.07ms
step:292/5960 train_time:280624ms step_avg:961.04ms
step:293/5960 train_time:281589ms step_avg:961.05ms
step:294/5960 train_time:282547ms step_avg:961.04ms
step:295/5960 train_time:283502ms step_avg:961.02ms
step:296/5960 train_time:284458ms step_avg:961.01ms
step:297/5960 train_time:285419ms step_avg:961.01ms
step:298/5960 train_time:286371ms step_avg:960.98ms
step:299/5960 train_time:287319ms step_avg:960.93ms
step:300/5960 train_time:288275ms step_avg:960.92ms
step:301/5960 train_time:289226ms step_avg:960.88ms
step:302/5960 train_time:290197ms step_avg:960.92ms
step:303/5960 train_time:291154ms step_avg:960.90ms
step:304/5960 train_time:292113ms step_avg:960.90ms
step:305/5960 train_time:293074ms step_avg:960.90ms
step:306/5960 train_time:294024ms step_avg:960.86ms
step:307/5960 train_time:294979ms step_avg:960.84ms
step:308/5960 train_time:295932ms step_avg:960.82ms
step:309/5960 train_time:296893ms step_avg:960.82ms
step:310/5960 train_time:297854ms step_avg:960.82ms
step:311/5960 train_time:298815ms step_avg:960.82ms
step:312/5960 train_time:299767ms step_avg:960.79ms
step:313/5960 train_time:300720ms step_avg:960.77ms
step:314/5960 train_time:301680ms step_avg:960.76ms
step:315/5960 train_time:302626ms step_avg:960.72ms
step:316/5960 train_time:303587ms step_avg:960.72ms
step:317/5960 train_time:304527ms step_avg:960.65ms
step:318/5960 train_time:305497ms step_avg:960.68ms
step:319/5960 train_time:306459ms step_avg:960.69ms
step:320/5960 train_time:307415ms step_avg:960.67ms
step:321/5960 train_time:308365ms step_avg:960.64ms
step:322/5960 train_time:309325ms step_avg:960.64ms
step:323/5960 train_time:310279ms step_avg:960.62ms
step:324/5960 train_time:311245ms step_avg:960.63ms
step:325/5960 train_time:312194ms step_avg:960.60ms
step:326/5960 train_time:313156ms step_avg:960.60ms
step:327/5960 train_time:314113ms step_avg:960.59ms
step:328/5960 train_time:315069ms step_avg:960.58ms
step:329/5960 train_time:316028ms step_avg:960.57ms
step:330/5960 train_time:316975ms step_avg:960.53ms
step:331/5960 train_time:317929ms step_avg:960.51ms
step:332/5960 train_time:318898ms step_avg:960.54ms
step:333/5960 train_time:319869ms step_avg:960.57ms
step:334/5960 train_time:320824ms step_avg:960.55ms
step:335/5960 train_time:321782ms step_avg:960.54ms
step:336/5960 train_time:322739ms step_avg:960.53ms
step:337/5960 train_time:323691ms step_avg:960.51ms
step:338/5960 train_time:324638ms step_avg:960.47ms
step:339/5960 train_time:325600ms step_avg:960.47ms
step:340/5960 train_time:326554ms step_avg:960.45ms
step:341/5960 train_time:327501ms step_avg:960.41ms
step:342/5960 train_time:328456ms step_avg:960.40ms
step:343/5960 train_time:329423ms step_avg:960.42ms
step:344/5960 train_time:330378ms step_avg:960.40ms
step:345/5960 train_time:331331ms step_avg:960.38ms
step:346/5960 train_time:332292ms step_avg:960.38ms
step:347/5960 train_time:333254ms step_avg:960.39ms
step:348/5960 train_time:334205ms step_avg:960.36ms
step:349/5960 train_time:335174ms step_avg:960.38ms
step:350/5960 train_time:336121ms step_avg:960.35ms
step:351/5960 train_time:337077ms step_avg:960.33ms
step:352/5960 train_time:338036ms step_avg:960.33ms
step:353/5960 train_time:338991ms step_avg:960.31ms
step:354/5960 train_time:339948ms step_avg:960.31ms
step:355/5960 train_time:340900ms step_avg:960.28ms
step:356/5960 train_time:341855ms step_avg:960.27ms
step:357/5960 train_time:342820ms step_avg:960.28ms
step:358/5960 train_time:343768ms step_avg:960.25ms
step:359/5960 train_time:344723ms step_avg:960.23ms
step:360/5960 train_time:345688ms step_avg:960.25ms
step:361/5960 train_time:346647ms step_avg:960.24ms
step:362/5960 train_time:347595ms step_avg:960.21ms
step:363/5960 train_time:348554ms step_avg:960.20ms
step:364/5960 train_time:349511ms step_avg:960.19ms
step:365/5960 train_time:350468ms step_avg:960.19ms
step:366/5960 train_time:351417ms step_avg:960.16ms
step:367/5960 train_time:352380ms step_avg:960.16ms
step:368/5960 train_time:353334ms step_avg:960.15ms
step:369/5960 train_time:354298ms step_avg:960.16ms
step:370/5960 train_time:355245ms step_avg:960.12ms
step:371/5960 train_time:356208ms step_avg:960.13ms
step:372/5960 train_time:357169ms step_avg:960.13ms
step:373/5960 train_time:358121ms step_avg:960.11ms
step:374/5960 train_time:359080ms step_avg:960.11ms
step:375/5960 train_time:360028ms step_avg:960.08ms
step:375/5960 val_loss:3.693420 train_time:360043ms step_avg:960.11ms
step:376/5960 train_time:360997ms step_avg:960.10ms
step:377/5960 train_time:361956ms step_avg:960.09ms
step:378/5960 train_time:362906ms step_avg:960.07ms
step:379/5960 train_time:363864ms step_avg:960.06ms
step:380/5960 train_time:364828ms step_avg:960.07ms
step:381/5960 train_time:365861ms step_avg:960.26ms
step:382/5960 train_time:366820ms step_avg:960.26ms
step:383/5960 train_time:367771ms step_avg:960.24ms
step:384/5960 train_time:368732ms step_avg:960.24ms
step:385/5960 train_time:369697ms step_avg:960.25ms
step:386/5960 train_time:370644ms step_avg:960.22ms
step:387/5960 train_time:371602ms step_avg:960.21ms
step:388/5960 train_time:372551ms step_avg:960.18ms
step:389/5960 train_time:373512ms step_avg:960.19ms
step:390/5960 train_time:374470ms step_avg:960.18ms
step:391/5960 train_time:375423ms step_avg:960.16ms
step:392/5960 train_time:376390ms step_avg:960.18ms
step:393/5960 train_time:377341ms step_avg:960.15ms
step:394/5960 train_time:378298ms step_avg:960.15ms
step:395/5960 train_time:379252ms step_avg:960.13ms
step:396/5960 train_time:380204ms step_avg:960.11ms
step:397/5960 train_time:381156ms step_avg:960.09ms
step:398/5960 train_time:382118ms step_avg:960.10ms
step:399/5960 train_time:383071ms step_avg:960.08ms
step:400/5960 train_time:384026ms step_avg:960.07ms
step:401/5960 train_time:384970ms step_avg:960.02ms
step:402/5960 train_time:385935ms step_avg:960.04ms
step:403/5960 train_time:386895ms step_avg:960.04ms
step:404/5960 train_time:387844ms step_avg:960.01ms
step:405/5960 train_time:388805ms step_avg:960.01ms
step:406/5960 train_time:389760ms step_avg:960.00ms
step:407/5960 train_time:390720ms step_avg:960.00ms
step:408/5960 train_time:391669ms step_avg:959.97ms
step:409/5960 train_time:392636ms step_avg:959.99ms
step:410/5960 train_time:393599ms step_avg:960.00ms
step:411/5960 train_time:394555ms step_avg:959.99ms
step:412/5960 train_time:395509ms step_avg:959.97ms
step:413/5960 train_time:396457ms step_avg:959.94ms
step:414/5960 train_time:397413ms step_avg:959.93ms
step:415/5960 train_time:398380ms step_avg:959.95ms
step:416/5960 train_time:399339ms step_avg:959.95ms
step:417/5960 train_time:400289ms step_avg:959.93ms
step:418/5960 train_time:401253ms step_avg:959.93ms
step:419/5960 train_time:402215ms step_avg:959.94ms
step:420/5960 train_time:403171ms step_avg:959.93ms
step:421/5960 train_time:404128ms step_avg:959.92ms
step:422/5960 train_time:405080ms step_avg:959.91ms
step:423/5960 train_time:406039ms step_avg:959.90ms
step:424/5960 train_time:407000ms step_avg:959.91ms
step:425/5960 train_time:407951ms step_avg:959.88ms
step:426/5960 train_time:408903ms step_avg:959.87ms
step:427/5960 train_time:409859ms step_avg:959.86ms
step:428/5960 train_time:410816ms step_avg:959.85ms
step:429/5960 train_time:411777ms step_avg:959.85ms
step:430/5960 train_time:412740ms step_avg:959.86ms
step:431/5960 train_time:413695ms step_avg:959.85ms
step:432/5960 train_time:414666ms step_avg:959.87ms
step:433/5960 train_time:415629ms step_avg:959.88ms
step:434/5960 train_time:416587ms step_avg:959.88ms
step:435/5960 train_time:417545ms step_avg:959.87ms
step:436/5960 train_time:418508ms step_avg:959.88ms
step:437/5960 train_time:419477ms step_avg:959.90ms
step:438/5960 train_time:420445ms step_avg:959.92ms
step:439/5960 train_time:421411ms step_avg:959.93ms
step:440/5960 train_time:422369ms step_avg:959.93ms
step:441/5960 train_time:423319ms step_avg:959.91ms
step:442/5960 train_time:424283ms step_avg:959.92ms
step:443/5960 train_time:425250ms step_avg:959.93ms
step:444/5960 train_time:426206ms step_avg:959.92ms
step:445/5960 train_time:427165ms step_avg:959.92ms
step:446/5960 train_time:428129ms step_avg:959.93ms
step:447/5960 train_time:429083ms step_avg:959.92ms
step:448/5960 train_time:430047ms step_avg:959.93ms
step:449/5960 train_time:431002ms step_avg:959.92ms
step:450/5960 train_time:431965ms step_avg:959.92ms
step:451/5960 train_time:432921ms step_avg:959.91ms
step:452/5960 train_time:433891ms step_avg:959.94ms
step:453/5960 train_time:434846ms step_avg:959.93ms
step:454/5960 train_time:435813ms step_avg:959.94ms
step:455/5960 train_time:436768ms step_avg:959.93ms
step:456/5960 train_time:437718ms step_avg:959.91ms
step:457/5960 train_time:438684ms step_avg:959.92ms
step:458/5960 train_time:439636ms step_avg:959.90ms
step:459/5960 train_time:440602ms step_avg:959.92ms
step:460/5960 train_time:441569ms step_avg:959.93ms
step:461/5960 train_time:442522ms step_avg:959.92ms
step:462/5960 train_time:443486ms step_avg:959.93ms
step:463/5960 train_time:444443ms step_avg:959.92ms
step:464/5960 train_time:445401ms step_avg:959.92ms
step:465/5960 train_time:446359ms step_avg:959.91ms
step:466/5960 train_time:447323ms step_avg:959.92ms
step:467/5960 train_time:448285ms step_avg:959.93ms
step:468/5960 train_time:449246ms step_avg:959.93ms
step:469/5960 train_time:450201ms step_avg:959.92ms
step:470/5960 train_time:451164ms step_avg:959.92ms
step:471/5960 train_time:452125ms step_avg:959.93ms
step:472/5960 train_time:453085ms step_avg:959.93ms
step:473/5960 train_time:454039ms step_avg:959.91ms
step:474/5960 train_time:455007ms step_avg:959.93ms
step:475/5960 train_time:455968ms step_avg:959.93ms
step:476/5960 train_time:456927ms step_avg:959.93ms
step:477/5960 train_time:457879ms step_avg:959.91ms
step:478/5960 train_time:458833ms step_avg:959.90ms
step:479/5960 train_time:459793ms step_avg:959.90ms
step:480/5960 train_time:460756ms step_avg:959.91ms
step:481/5960 train_time:461725ms step_avg:959.93ms
step:482/5960 train_time:462692ms step_avg:959.94ms
step:483/5960 train_time:463644ms step_avg:959.93ms
step:484/5960 train_time:464611ms step_avg:959.94ms
step:485/5960 train_time:465563ms step_avg:959.92ms
step:486/5960 train_time:466540ms step_avg:959.96ms
step:487/5960 train_time:467496ms step_avg:959.95ms
step:488/5960 train_time:468454ms step_avg:959.95ms
step:489/5960 train_time:469415ms step_avg:959.95ms
step:490/5960 train_time:470372ms step_avg:959.94ms
step:491/5960 train_time:471324ms step_avg:959.93ms
step:492/5960 train_time:472289ms step_avg:959.94ms
step:493/5960 train_time:473246ms step_avg:959.93ms
step:494/5960 train_time:474196ms step_avg:959.91ms
step:495/5960 train_time:475166ms step_avg:959.93ms
step:496/5960 train_time:476133ms step_avg:959.95ms
step:497/5960 train_time:477091ms step_avg:959.94ms
step:498/5960 train_time:478050ms step_avg:959.94ms
step:499/5960 train_time:479008ms step_avg:959.94ms
step:500/5960 train_time:479964ms step_avg:959.93ms
step:500/5960 val_loss:3.577090 train_time:479975ms step_avg:959.95ms
step:501/5960 train_time:480937ms step_avg:959.95ms
step:502/5960 train_time:481894ms step_avg:959.95ms
step:503/5960 train_time:482859ms step_avg:959.96ms
step:504/5960 train_time:483824ms step_avg:959.97ms
step:505/5960 train_time:484784ms step_avg:959.97ms
step:506/5960 train_time:485754ms step_avg:959.99ms
step:507/5960 train_time:486712ms step_avg:959.98ms
step:508/5960 train_time:487672ms step_avg:959.98ms
step:509/5960 train_time:488632ms step_avg:959.98ms
step:510/5960 train_time:489586ms step_avg:959.97ms
step:511/5960 train_time:490555ms step_avg:959.99ms
step:512/5960 train_time:491510ms step_avg:959.98ms
step:513/5960 train_time:492470ms step_avg:959.98ms
step:514/5960 train_time:493438ms step_avg:960.00ms
step:515/5960 train_time:494394ms step_avg:959.99ms
step:516/5960 train_time:495370ms step_avg:960.02ms
step:517/5960 train_time:496324ms step_avg:960.01ms
step:518/5960 train_time:497287ms step_avg:960.01ms
step:519/5960 train_time:498252ms step_avg:960.02ms
step:520/5960 train_time:499211ms step_avg:960.02ms
step:521/5960 train_time:500167ms step_avg:960.01ms
step:522/5960 train_time:501124ms step_avg:960.01ms
step:523/5960 train_time:502084ms step_avg:960.01ms
step:524/5960 train_time:503040ms step_avg:960.00ms
step:525/5960 train_time:504007ms step_avg:960.01ms
step:526/5960 train_time:504960ms step_avg:960.00ms
step:527/5960 train_time:505931ms step_avg:960.02ms
step:528/5960 train_time:506892ms step_avg:960.02ms
step:529/5960 train_time:507843ms step_avg:960.00ms
step:530/5960 train_time:508814ms step_avg:960.03ms
step:531/5960 train_time:509771ms step_avg:960.02ms
step:532/5960 train_time:510731ms step_avg:960.02ms
step:533/5960 train_time:511687ms step_avg:960.01ms
step:534/5960 train_time:512652ms step_avg:960.02ms
step:535/5960 train_time:513610ms step_avg:960.02ms
step:536/5960 train_time:514573ms step_avg:960.02ms
step:537/5960 train_time:515541ms step_avg:960.04ms
step:538/5960 train_time:516493ms step_avg:960.02ms
step:539/5960 train_time:517457ms step_avg:960.03ms
step:540/5960 train_time:518424ms step_avg:960.04ms
step:541/5960 train_time:519388ms step_avg:960.05ms
step:542/5960 train_time:520351ms step_avg:960.06ms
step:543/5960 train_time:521309ms step_avg:960.05ms
step:544/5960 train_time:522272ms step_avg:960.06ms
step:545/5960 train_time:523238ms step_avg:960.07ms
step:546/5960 train_time:524190ms step_avg:960.06ms
step:547/5960 train_time:525156ms step_avg:960.07ms
step:548/5960 train_time:526113ms step_avg:960.06ms
step:549/5960 train_time:527074ms step_avg:960.06ms
step:550/5960 train_time:528031ms step_avg:960.06ms
step:551/5960 train_time:528992ms step_avg:960.06ms
step:552/5960 train_time:529955ms step_avg:960.06ms
step:553/5960 train_time:530910ms step_avg:960.05ms
step:554/5960 train_time:531887ms step_avg:960.08ms
step:555/5960 train_time:532840ms step_avg:960.07ms
step:556/5960 train_time:533799ms step_avg:960.07ms
step:557/5960 train_time:534771ms step_avg:960.09ms
step:558/5960 train_time:535723ms step_avg:960.08ms
step:559/5960 train_time:536695ms step_avg:960.10ms
step:560/5960 train_time:537651ms step_avg:960.09ms
step:561/5960 train_time:538617ms step_avg:960.10ms
step:562/5960 train_time:539578ms step_avg:960.10ms
step:563/5960 train_time:540535ms step_avg:960.10ms
step:564/5960 train_time:541492ms step_avg:960.09ms
step:565/5960 train_time:542451ms step_avg:960.09ms
step:566/5960 train_time:543414ms step_avg:960.10ms
step:567/5960 train_time:544374ms step_avg:960.09ms
step:568/5960 train_time:545340ms step_avg:960.11ms
step:569/5960 train_time:546300ms step_avg:960.10ms
step:570/5960 train_time:547263ms step_avg:960.11ms
step:571/5960 train_time:548271ms step_avg:960.19ms
step:572/5960 train_time:549243ms step_avg:960.21ms
step:573/5960 train_time:550190ms step_avg:960.19ms
step:574/5960 train_time:551158ms step_avg:960.21ms
step:575/5960 train_time:552119ms step_avg:960.21ms
step:576/5960 train_time:553087ms step_avg:960.22ms
step:577/5960 train_time:554047ms step_avg:960.22ms
step:578/5960 train_time:554999ms step_avg:960.21ms
step:579/5960 train_time:555970ms step_avg:960.22ms
step:580/5960 train_time:556930ms step_avg:960.22ms
step:581/5960 train_time:557883ms step_avg:960.21ms
step:582/5960 train_time:558846ms step_avg:960.22ms
step:583/5960 train_time:559805ms step_avg:960.22ms
step:584/5960 train_time:560765ms step_avg:960.21ms
step:585/5960 train_time:561733ms step_avg:960.23ms
step:586/5960 train_time:562685ms step_avg:960.21ms
step:587/5960 train_time:563645ms step_avg:960.21ms
step:588/5960 train_time:564597ms step_avg:960.20ms
step:589/5960 train_time:565566ms step_avg:960.21ms
step:590/5960 train_time:566521ms step_avg:960.21ms
step:591/5960 train_time:567484ms step_avg:960.21ms
step:592/5960 train_time:568445ms step_avg:960.21ms
step:593/5960 train_time:569410ms step_avg:960.22ms
step:594/5960 train_time:570365ms step_avg:960.21ms
step:595/5960 train_time:571330ms step_avg:960.22ms
step:596/5960 train_time:572282ms step_avg:960.20ms
step:597/5960 train_time:573246ms step_avg:960.21ms
step:598/5960 train_time:574204ms step_avg:960.21ms
step:599/5960 train_time:575166ms step_avg:960.21ms
step:600/5960 train_time:576139ms step_avg:960.23ms
step:601/5960 train_time:577095ms step_avg:960.23ms
step:602/5960 train_time:578055ms step_avg:960.22ms
step:603/5960 train_time:579017ms step_avg:960.23ms
step:604/5960 train_time:579972ms step_avg:960.22ms
step:605/5960 train_time:580944ms step_avg:960.24ms
step:606/5960 train_time:581900ms step_avg:960.23ms
step:607/5960 train_time:582856ms step_avg:960.22ms
step:608/5960 train_time:583822ms step_avg:960.23ms
step:609/5960 train_time:584783ms step_avg:960.23ms
step:610/5960 train_time:585736ms step_avg:960.22ms
step:611/5960 train_time:586711ms step_avg:960.25ms
step:612/5960 train_time:587670ms step_avg:960.25ms
step:613/5960 train_time:588634ms step_avg:960.25ms
step:614/5960 train_time:589585ms step_avg:960.24ms
step:615/5960 train_time:590549ms step_avg:960.24ms
step:616/5960 train_time:591503ms step_avg:960.23ms
step:617/5960 train_time:592460ms step_avg:960.23ms
step:618/5960 train_time:593426ms step_avg:960.24ms
step:619/5960 train_time:594378ms step_avg:960.22ms
step:620/5960 train_time:595345ms step_avg:960.23ms
step:621/5960 train_time:596308ms step_avg:960.24ms
step:622/5960 train_time:597262ms step_avg:960.23ms
step:623/5960 train_time:598224ms step_avg:960.23ms
step:624/5960 train_time:599190ms step_avg:960.24ms
step:625/5960 train_time:600153ms step_avg:960.24ms
step:625/5960 val_loss:3.499849 train_time:600160ms step_avg:960.26ms
step:626/5960 train_time:601119ms step_avg:960.25ms
step:627/5960 train_time:602080ms step_avg:960.26ms
step:628/5960 train_time:603049ms step_avg:960.27ms
step:629/5960 train_time:604008ms step_avg:960.27ms
step:630/5960 train_time:604963ms step_avg:960.26ms
step:631/5960 train_time:605926ms step_avg:960.26ms
step:632/5960 train_time:606880ms step_avg:960.25ms
step:633/5960 train_time:607846ms step_avg:960.26ms
step:634/5960 train_time:608811ms step_avg:960.27ms
step:635/5960 train_time:609777ms step_avg:960.28ms
step:636/5960 train_time:610724ms step_avg:960.26ms
step:637/5960 train_time:611689ms step_avg:960.27ms
step:638/5960 train_time:612644ms step_avg:960.26ms
step:639/5960 train_time:613605ms step_avg:960.26ms
step:640/5960 train_time:614558ms step_avg:960.25ms
step:641/5960 train_time:615525ms step_avg:960.26ms
step:642/5960 train_time:616492ms step_avg:960.27ms
step:643/5960 train_time:617454ms step_avg:960.27ms
step:644/5960 train_time:618405ms step_avg:960.26ms
step:645/5960 train_time:619370ms step_avg:960.26ms
step:646/5960 train_time:620340ms step_avg:960.28ms
step:647/5960 train_time:621303ms step_avg:960.28ms
step:648/5960 train_time:622265ms step_avg:960.29ms
step:649/5960 train_time:623220ms step_avg:960.28ms
step:650/5960 train_time:624178ms step_avg:960.27ms
step:651/5960 train_time:625142ms step_avg:960.28ms
step:652/5960 train_time:626112ms step_avg:960.30ms
step:653/5960 train_time:627073ms step_avg:960.29ms
step:654/5960 train_time:628039ms step_avg:960.30ms
step:655/5960 train_time:629002ms step_avg:960.31ms
step:656/5960 train_time:629963ms step_avg:960.31ms
step:657/5960 train_time:630921ms step_avg:960.31ms
step:658/5960 train_time:631884ms step_avg:960.31ms
step:659/5960 train_time:632850ms step_avg:960.32ms
step:660/5960 train_time:633812ms step_avg:960.32ms
step:661/5960 train_time:634783ms step_avg:960.34ms
step:662/5960 train_time:635740ms step_avg:960.33ms
step:663/5960 train_time:636707ms step_avg:960.34ms
step:664/5960 train_time:637673ms step_avg:960.35ms
step:665/5960 train_time:638633ms step_avg:960.35ms
step:666/5960 train_time:639604ms step_avg:960.37ms
step:667/5960 train_time:640567ms step_avg:960.37ms
step:668/5960 train_time:641530ms step_avg:960.37ms
step:669/5960 train_time:642501ms step_avg:960.39ms
step:670/5960 train_time:643461ms step_avg:960.39ms
step:671/5960 train_time:644413ms step_avg:960.38ms
step:672/5960 train_time:645383ms step_avg:960.39ms
step:673/5960 train_time:646350ms step_avg:960.40ms
step:674/5960 train_time:647307ms step_avg:960.40ms
step:675/5960 train_time:648276ms step_avg:960.41ms
step:676/5960 train_time:649238ms step_avg:960.41ms
step:677/5960 train_time:650204ms step_avg:960.42ms
step:678/5960 train_time:651177ms step_avg:960.44ms
step:679/5960 train_time:652137ms step_avg:960.44ms
step:680/5960 train_time:653101ms step_avg:960.44ms
step:681/5960 train_time:654065ms step_avg:960.45ms
step:682/5960 train_time:655033ms step_avg:960.46ms
step:683/5960 train_time:656006ms step_avg:960.48ms
step:684/5960 train_time:656966ms step_avg:960.48ms
step:685/5960 train_time:657931ms step_avg:960.48ms
step:686/5960 train_time:658898ms step_avg:960.49ms
step:687/5960 train_time:659857ms step_avg:960.49ms
step:688/5960 train_time:660821ms step_avg:960.50ms
step:689/5960 train_time:661790ms step_avg:960.51ms
step:690/5960 train_time:662753ms step_avg:960.51ms
step:691/5960 train_time:663710ms step_avg:960.51ms
step:692/5960 train_time:664680ms step_avg:960.52ms
step:693/5960 train_time:665648ms step_avg:960.53ms
step:694/5960 train_time:666606ms step_avg:960.53ms
step:695/5960 train_time:667569ms step_avg:960.53ms
step:696/5960 train_time:668525ms step_avg:960.52ms
step:697/5960 train_time:669497ms step_avg:960.54ms
step:698/5960 train_time:670452ms step_avg:960.53ms
step:699/5960 train_time:671428ms step_avg:960.55ms
step:700/5960 train_time:672386ms step_avg:960.55ms
step:701/5960 train_time:673356ms step_avg:960.57ms
step:702/5960 train_time:674314ms step_avg:960.56ms
step:703/5960 train_time:675273ms step_avg:960.56ms
step:704/5960 train_time:676234ms step_avg:960.56ms
step:705/5960 train_time:677213ms step_avg:960.59ms
step:706/5960 train_time:678174ms step_avg:960.59ms
step:707/5960 train_time:679127ms step_avg:960.58ms
step:708/5960 train_time:680108ms step_avg:960.60ms
step:709/5960 train_time:681063ms step_avg:960.60ms
step:710/5960 train_time:682037ms step_avg:960.62ms
step:711/5960 train_time:682993ms step_avg:960.61ms
step:712/5960 train_time:683970ms step_avg:960.63ms
step:713/5960 train_time:684928ms step_avg:960.63ms
step:714/5960 train_time:685895ms step_avg:960.64ms
step:715/5960 train_time:686858ms step_avg:960.64ms
step:716/5960 train_time:687809ms step_avg:960.63ms
step:717/5960 train_time:688794ms step_avg:960.66ms
step:718/5960 train_time:689758ms step_avg:960.67ms
step:719/5960 train_time:690712ms step_avg:960.66ms
step:720/5960 train_time:691674ms step_avg:960.66ms
step:721/5960 train_time:692639ms step_avg:960.66ms
step:722/5960 train_time:693609ms step_avg:960.68ms
step:723/5960 train_time:694566ms step_avg:960.67ms
step:724/5960 train_time:695540ms step_avg:960.69ms
step:725/5960 train_time:696498ms step_avg:960.69ms
step:726/5960 train_time:697465ms step_avg:960.70ms
step:727/5960 train_time:698429ms step_avg:960.70ms
step:728/5960 train_time:699386ms step_avg:960.69ms
step:729/5960 train_time:700358ms step_avg:960.71ms
step:730/5960 train_time:701316ms step_avg:960.71ms
step:731/5960 train_time:702291ms step_avg:960.73ms
step:732/5960 train_time:703263ms step_avg:960.74ms
step:733/5960 train_time:704220ms step_avg:960.74ms
step:734/5960 train_time:705188ms step_avg:960.75ms
step:735/5960 train_time:706154ms step_avg:960.75ms
step:736/5960 train_time:707111ms step_avg:960.75ms
step:737/5960 train_time:708090ms step_avg:960.77ms
step:738/5960 train_time:709057ms step_avg:960.78ms
step:739/5960 train_time:710018ms step_avg:960.78ms
step:740/5960 train_time:710981ms step_avg:960.78ms
step:741/5960 train_time:711944ms step_avg:960.79ms
step:742/5960 train_time:712910ms step_avg:960.80ms
step:743/5960 train_time:713876ms step_avg:960.80ms
step:744/5960 train_time:714838ms step_avg:960.80ms
step:745/5960 train_time:715801ms step_avg:960.81ms
step:746/5960 train_time:716766ms step_avg:960.81ms
step:747/5960 train_time:717725ms step_avg:960.81ms
step:748/5960 train_time:718696ms step_avg:960.82ms
step:749/5960 train_time:719675ms step_avg:960.85ms
step:750/5960 train_time:720638ms step_avg:960.85ms
step:750/5960 val_loss:3.444866 train_time:720643ms step_avg:960.86ms
step:751/5960 train_time:721601ms step_avg:960.85ms
step:752/5960 train_time:722565ms step_avg:960.86ms
step:753/5960 train_time:723534ms step_avg:960.87ms
step:754/5960 train_time:724508ms step_avg:960.89ms
step:755/5960 train_time:725464ms step_avg:960.88ms
step:756/5960 train_time:726423ms step_avg:960.88ms
step:757/5960 train_time:727398ms step_avg:960.90ms
step:758/5960 train_time:728363ms step_avg:960.90ms
step:759/5960 train_time:729324ms step_avg:960.90ms
step:760/5960 train_time:730290ms step_avg:960.91ms
step:761/5960 train_time:731311ms step_avg:960.99ms
step:762/5960 train_time:732278ms step_avg:961.00ms
step:763/5960 train_time:733238ms step_avg:960.99ms
step:764/5960 train_time:734197ms step_avg:960.99ms
step:765/5960 train_time:735176ms step_avg:961.01ms
step:766/5960 train_time:736139ms step_avg:961.02ms
step:767/5960 train_time:737095ms step_avg:961.01ms
step:768/5960 train_time:738058ms step_avg:961.01ms
step:769/5960 train_time:739025ms step_avg:961.02ms
step:770/5960 train_time:739988ms step_avg:961.02ms
step:771/5960 train_time:740950ms step_avg:961.02ms
step:772/5960 train_time:741914ms step_avg:961.03ms
step:773/5960 train_time:742879ms step_avg:961.03ms
step:774/5960 train_time:743851ms step_avg:961.05ms
step:775/5960 train_time:744808ms step_avg:961.04ms
step:776/5960 train_time:745772ms step_avg:961.05ms
step:777/5960 train_time:746724ms step_avg:961.03ms
step:778/5960 train_time:747695ms step_avg:961.05ms
step:779/5960 train_time:748668ms step_avg:961.06ms
step:780/5960 train_time:749639ms step_avg:961.08ms
step:781/5960 train_time:750599ms step_avg:961.07ms
step:782/5960 train_time:751567ms step_avg:961.08ms
step:783/5960 train_time:752529ms step_avg:961.08ms
step:784/5960 train_time:753487ms step_avg:961.08ms
step:785/5960 train_time:754452ms step_avg:961.09ms
step:786/5960 train_time:755418ms step_avg:961.09ms
step:787/5960 train_time:756384ms step_avg:961.10ms
step:788/5960 train_time:757356ms step_avg:961.11ms
step:789/5960 train_time:758320ms step_avg:961.11ms
step:790/5960 train_time:759282ms step_avg:961.12ms
step:791/5960 train_time:760245ms step_avg:961.12ms
step:792/5960 train_time:761214ms step_avg:961.13ms
step:793/5960 train_time:762179ms step_avg:961.13ms
step:794/5960 train_time:763144ms step_avg:961.14ms
step:795/5960 train_time:764112ms step_avg:961.15ms
step:796/5960 train_time:765074ms step_avg:961.15ms
step:797/5960 train_time:766046ms step_avg:961.16ms
step:798/5960 train_time:767013ms step_avg:961.17ms
step:799/5960 train_time:767978ms step_avg:961.17ms
step:800/5960 train_time:768953ms step_avg:961.19ms
step:801/5960 train_time:769916ms step_avg:961.19ms
step:802/5960 train_time:770880ms step_avg:961.20ms
step:803/5960 train_time:771841ms step_avg:961.20ms
step:804/5960 train_time:772811ms step_avg:961.21ms
step:805/5960 train_time:773786ms step_avg:961.22ms
step:806/5960 train_time:774750ms step_avg:961.23ms
step:807/5960 train_time:775713ms step_avg:961.23ms
step:808/5960 train_time:776668ms step_avg:961.22ms
step:809/5960 train_time:777639ms step_avg:961.24ms
step:810/5960 train_time:778598ms step_avg:961.23ms
step:811/5960 train_time:779566ms step_avg:961.24ms
step:812/5960 train_time:780526ms step_avg:961.24ms
step:813/5960 train_time:781494ms step_avg:961.25ms
step:814/5960 train_time:782454ms step_avg:961.25ms
step:815/5960 train_time:783409ms step_avg:961.24ms
step:816/5960 train_time:784385ms step_avg:961.26ms
step:817/5960 train_time:785360ms step_avg:961.27ms
step:818/5960 train_time:786310ms step_avg:961.26ms
step:819/5960 train_time:787288ms step_avg:961.28ms
step:820/5960 train_time:788253ms step_avg:961.28ms
step:821/5960 train_time:789212ms step_avg:961.28ms
step:822/5960 train_time:790177ms step_avg:961.29ms
step:823/5960 train_time:791150ms step_avg:961.30ms
step:824/5960 train_time:792105ms step_avg:961.29ms
step:825/5960 train_time:793081ms step_avg:961.31ms
step:826/5960 train_time:794050ms step_avg:961.32ms
step:827/5960 train_time:795014ms step_avg:961.32ms
step:828/5960 train_time:795973ms step_avg:961.32ms
step:829/5960 train_time:796938ms step_avg:961.32ms
step:830/5960 train_time:797906ms step_avg:961.33ms
step:831/5960 train_time:798866ms step_avg:961.33ms
step:832/5960 train_time:799836ms step_avg:961.34ms
step:833/5960 train_time:800789ms step_avg:961.33ms
step:834/5960 train_time:801765ms step_avg:961.35ms
step:835/5960 train_time:802728ms step_avg:961.35ms
step:836/5960 train_time:803696ms step_avg:961.36ms
step:837/5960 train_time:804662ms step_avg:961.36ms
step:838/5960 train_time:805615ms step_avg:961.35ms
step:839/5960 train_time:806577ms step_avg:961.36ms
step:840/5960 train_time:807547ms step_avg:961.37ms
step:841/5960 train_time:808511ms step_avg:961.37ms
step:842/5960 train_time:809474ms step_avg:961.37ms
step:843/5960 train_time:810437ms step_avg:961.37ms
step:844/5960 train_time:811400ms step_avg:961.37ms
step:845/5960 train_time:812363ms step_avg:961.38ms
step:846/5960 train_time:813327ms step_avg:961.38ms
step:847/5960 train_time:814291ms step_avg:961.38ms
step:848/5960 train_time:815258ms step_avg:961.39ms
step:849/5960 train_time:816219ms step_avg:961.39ms
step:850/5960 train_time:817185ms step_avg:961.39ms
step:851/5960 train_time:818152ms step_avg:961.40ms
step:852/5960 train_time:819118ms step_avg:961.41ms
step:853/5960 train_time:820076ms step_avg:961.40ms
step:854/5960 train_time:821048ms step_avg:961.41ms
step:855/5960 train_time:822012ms step_avg:961.42ms
step:856/5960 train_time:822976ms step_avg:961.42ms
step:857/5960 train_time:823946ms step_avg:961.43ms
step:858/5960 train_time:824917ms step_avg:961.44ms
step:859/5960 train_time:825885ms step_avg:961.45ms
step:860/5960 train_time:826862ms step_avg:961.47ms
step:861/5960 train_time:827821ms step_avg:961.47ms
step:862/5960 train_time:828787ms step_avg:961.47ms
step:863/5960 train_time:829759ms step_avg:961.48ms
step:864/5960 train_time:830714ms step_avg:961.47ms
step:865/5960 train_time:831678ms step_avg:961.48ms
step:866/5960 train_time:832645ms step_avg:961.48ms
step:867/5960 train_time:833614ms step_avg:961.49ms
step:868/5960 train_time:834582ms step_avg:961.50ms
step:869/5960 train_time:835536ms step_avg:961.49ms
step:870/5960 train_time:836511ms step_avg:961.51ms
step:871/5960 train_time:837463ms step_avg:961.50ms
step:872/5960 train_time:838428ms step_avg:961.50ms
step:873/5960 train_time:839391ms step_avg:961.50ms
step:874/5960 train_time:840359ms step_avg:961.51ms
step:875/5960 train_time:841318ms step_avg:961.51ms
step:875/5960 val_loss:3.399759 train_time:841323ms step_avg:961.51ms
step:876/5960 train_time:842287ms step_avg:961.51ms
step:877/5960 train_time:843250ms step_avg:961.52ms
step:878/5960 train_time:844209ms step_avg:961.51ms
step:879/5960 train_time:845176ms step_avg:961.52ms
step:880/5960 train_time:846134ms step_avg:961.52ms
step:881/5960 train_time:847101ms step_avg:961.52ms
step:882/5960 train_time:848068ms step_avg:961.53ms
step:883/5960 train_time:849022ms step_avg:961.52ms
step:884/5960 train_time:849986ms step_avg:961.52ms
step:885/5960 train_time:850951ms step_avg:961.53ms
step:886/5960 train_time:851916ms step_avg:961.53ms
step:887/5960 train_time:852874ms step_avg:961.53ms
step:888/5960 train_time:853840ms step_avg:961.53ms
step:889/5960 train_time:854806ms step_avg:961.54ms
step:890/5960 train_time:855779ms step_avg:961.55ms
step:891/5960 train_time:856740ms step_avg:961.55ms
step:892/5960 train_time:857699ms step_avg:961.55ms
step:893/5960 train_time:858662ms step_avg:961.55ms
step:894/5960 train_time:859631ms step_avg:961.56ms
step:895/5960 train_time:860595ms step_avg:961.56ms
step:896/5960 train_time:861561ms step_avg:961.56ms
step:897/5960 train_time:862517ms step_avg:961.56ms
step:898/5960 train_time:863496ms step_avg:961.58ms
step:899/5960 train_time:864455ms step_avg:961.57ms
step:900/5960 train_time:865424ms step_avg:961.58ms
step:901/5960 train_time:866397ms step_avg:961.59ms
step:902/5960 train_time:867359ms step_avg:961.59ms
step:903/5960 train_time:868325ms step_avg:961.60ms
step:904/5960 train_time:869290ms step_avg:961.60ms
step:905/5960 train_time:870245ms step_avg:961.60ms
step:906/5960 train_time:871214ms step_avg:961.60ms
step:907/5960 train_time:872177ms step_avg:961.61ms
step:908/5960 train_time:873145ms step_avg:961.61ms
step:909/5960 train_time:874116ms step_avg:961.62ms
step:910/5960 train_time:875073ms step_avg:961.62ms
step:911/5960 train_time:876035ms step_avg:961.62ms
step:912/5960 train_time:876998ms step_avg:961.62ms
step:913/5960 train_time:877981ms step_avg:961.64ms
step:914/5960 train_time:878936ms step_avg:961.64ms
step:915/5960 train_time:879902ms step_avg:961.64ms
step:916/5960 train_time:880873ms step_avg:961.65ms
step:917/5960 train_time:881835ms step_avg:961.65ms
step:918/5960 train_time:882804ms step_avg:961.66ms
step:919/5960 train_time:883781ms step_avg:961.68ms
step:920/5960 train_time:884743ms step_avg:961.68ms
step:921/5960 train_time:885709ms step_avg:961.68ms
step:922/5960 train_time:886685ms step_avg:961.70ms
step:923/5960 train_time:887649ms step_avg:961.70ms
step:924/5960 train_time:888617ms step_avg:961.71ms
step:925/5960 train_time:889575ms step_avg:961.70ms
step:926/5960 train_time:890548ms step_avg:961.72ms
step:927/5960 train_time:891510ms step_avg:961.72ms
step:928/5960 train_time:892475ms step_avg:961.72ms
step:929/5960 train_time:893441ms step_avg:961.72ms
step:930/5960 train_time:894417ms step_avg:961.74ms
step:931/5960 train_time:895382ms step_avg:961.74ms
step:932/5960 train_time:896349ms step_avg:961.75ms
step:933/5960 train_time:897307ms step_avg:961.74ms
step:934/5960 train_time:898275ms step_avg:961.75ms
step:935/5960 train_time:899247ms step_avg:961.76ms
step:936/5960 train_time:900219ms step_avg:961.77ms
step:937/5960 train_time:901181ms step_avg:961.77ms
step:938/5960 train_time:902149ms step_avg:961.78ms
step:939/5960 train_time:903110ms step_avg:961.78ms
step:940/5960 train_time:904080ms step_avg:961.79ms
step:941/5960 train_time:905048ms step_avg:961.79ms
step:942/5960 train_time:906022ms step_avg:961.81ms
step:943/5960 train_time:906995ms step_avg:961.82ms
step:944/5960 train_time:907959ms step_avg:961.82ms
step:945/5960 train_time:908928ms step_avg:961.83ms
step:946/5960 train_time:909895ms step_avg:961.83ms
step:947/5960 train_time:910856ms step_avg:961.83ms
step:948/5960 train_time:911823ms step_avg:961.84ms
step:949/5960 train_time:912794ms step_avg:961.85ms
step:950/5960 train_time:913765ms step_avg:961.86ms
step:951/5960 train_time:914796ms step_avg:961.93ms
step:952/5960 train_time:915776ms step_avg:961.95ms
step:953/5960 train_time:916737ms step_avg:961.95ms
step:954/5960 train_time:917701ms step_avg:961.95ms
step:955/5960 train_time:918677ms step_avg:961.97ms
step:956/5960 train_time:919645ms step_avg:961.97ms
step:957/5960 train_time:920615ms step_avg:961.98ms
step:958/5960 train_time:921590ms step_avg:961.99ms
step:959/5960 train_time:922556ms step_avg:962.00ms
step:960/5960 train_time:923519ms step_avg:962.00ms
step:961/5960 train_time:924495ms step_avg:962.01ms
step:962/5960 train_time:925455ms step_avg:962.01ms
step:963/5960 train_time:926425ms step_avg:962.02ms
step:964/5960 train_time:927409ms step_avg:962.04ms
step:965/5960 train_time:928365ms step_avg:962.04ms
step:966/5960 train_time:929337ms step_avg:962.05ms
step:967/5960 train_time:930302ms step_avg:962.05ms
step:968/5960 train_time:931281ms step_avg:962.07ms
step:969/5960 train_time:932248ms step_avg:962.07ms
step:970/5960 train_time:933222ms step_avg:962.08ms
step:971/5960 train_time:934189ms step_avg:962.09ms
step:972/5960 train_time:935154ms step_avg:962.09ms
step:973/5960 train_time:936112ms step_avg:962.09ms
step:974/5960 train_time:937085ms step_avg:962.10ms
step:975/5960 train_time:938057ms step_avg:962.11ms
step:976/5960 train_time:939019ms step_avg:962.11ms
step:977/5960 train_time:939987ms step_avg:962.12ms
step:978/5960 train_time:940955ms step_avg:962.12ms
step:979/5960 train_time:941929ms step_avg:962.13ms
step:980/5960 train_time:942896ms step_avg:962.14ms
step:981/5960 train_time:943867ms step_avg:962.15ms
step:982/5960 train_time:944830ms step_avg:962.15ms
step:983/5960 train_time:945796ms step_avg:962.15ms
step:984/5960 train_time:946763ms step_avg:962.16ms
step:985/5960 train_time:947728ms step_avg:962.16ms
step:986/5960 train_time:948700ms step_avg:962.17ms
step:987/5960 train_time:949666ms step_avg:962.17ms
step:988/5960 train_time:950626ms step_avg:962.17ms
step:989/5960 train_time:951594ms step_avg:962.18ms
step:990/5960 train_time:952563ms step_avg:962.19ms
step:991/5960 train_time:953532ms step_avg:962.19ms
step:992/5960 train_time:954508ms step_avg:962.21ms
step:993/5960 train_time:955478ms step_avg:962.21ms
step:994/5960 train_time:956449ms step_avg:962.22ms
step:995/5960 train_time:957416ms step_avg:962.23ms
step:996/5960 train_time:958376ms step_avg:962.23ms
step:997/5960 train_time:959341ms step_avg:962.23ms
step:998/5960 train_time:960313ms step_avg:962.24ms
step:999/5960 train_time:961282ms step_avg:962.24ms
step:1000/5960 train_time:962247ms step_avg:962.25ms
step:1000/5960 val_loss:3.361335 train_time:962247ms step_avg:962.25ms
step:1001/5960 train_time:963213ms step_avg:962.25ms
step:1002/5960 train_time:964176ms step_avg:962.25ms
step:1003/5960 train_time:965149ms step_avg:962.26ms
step:1004/5960 train_time:966107ms step_avg:962.26ms
step:1005/5960 train_time:967082ms step_avg:962.27ms
step:1006/5960 train_time:968046ms step_avg:962.27ms
step:1007/5960 train_time:969009ms step_avg:962.27ms
step:1008/5960 train_time:969990ms step_avg:962.29ms
step:1009/5960 train_time:970950ms step_avg:962.29ms
step:1010/5960 train_time:971922ms step_avg:962.30ms
step:1011/5960 train_time:972893ms step_avg:962.31ms
step:1012/5960 train_time:973852ms step_avg:962.30ms
step:1013/5960 train_time:974823ms step_avg:962.31ms
step:1014/5960 train_time:975785ms step_avg:962.31ms
step:1015/5960 train_time:976756ms step_avg:962.32ms
step:1016/5960 train_time:977731ms step_avg:962.33ms
step:1017/5960 train_time:978699ms step_avg:962.34ms
step:1018/5960 train_time:979677ms step_avg:962.35ms
step:1019/5960 train_time:980651ms step_avg:962.37ms
step:1020/5960 train_time:981628ms step_avg:962.38ms
step:1021/5960 train_time:982592ms step_avg:962.38ms
step:1022/5960 train_time:983569ms step_avg:962.40ms
step:1023/5960 train_time:984535ms step_avg:962.40ms
step:1024/5960 train_time:985501ms step_avg:962.40ms
step:1025/5960 train_time:986473ms step_avg:962.41ms
step:1026/5960 train_time:987440ms step_avg:962.42ms
step:1027/5960 train_time:988410ms step_avg:962.42ms
step:1028/5960 train_time:989381ms step_avg:962.43ms
step:1029/5960 train_time:990347ms step_avg:962.44ms
step:1030/5960 train_time:991316ms step_avg:962.44ms
step:1031/5960 train_time:992292ms step_avg:962.46ms
step:1032/5960 train_time:993252ms step_avg:962.45ms
step:1033/5960 train_time:994218ms step_avg:962.46ms
step:1034/5960 train_time:995194ms step_avg:962.47ms
step:1035/5960 train_time:996169ms step_avg:962.48ms
step:1036/5960 train_time:997141ms step_avg:962.49ms
step:1037/5960 train_time:998119ms step_avg:962.51ms
step:1038/5960 train_time:999093ms step_avg:962.52ms
step:1039/5960 train_time:1000052ms step_avg:962.51ms
step:1040/5960 train_time:1001026ms step_avg:962.52ms
step:1041/5960 train_time:1001993ms step_avg:962.53ms
step:1042/5960 train_time:1002956ms step_avg:962.53ms
step:1043/5960 train_time:1003929ms step_avg:962.54ms
step:1044/5960 train_time:1004894ms step_avg:962.54ms
step:1045/5960 train_time:1005857ms step_avg:962.54ms
step:1046/5960 train_time:1006816ms step_avg:962.54ms
step:1047/5960 train_time:1007787ms step_avg:962.55ms
step:1048/5960 train_time:1008749ms step_avg:962.55ms
step:1049/5960 train_time:1009722ms step_avg:962.56ms
step:1050/5960 train_time:1010692ms step_avg:962.56ms
step:1051/5960 train_time:1011664ms step_avg:962.57ms
step:1052/5960 train_time:1012630ms step_avg:962.58ms
step:1053/5960 train_time:1013592ms step_avg:962.58ms
step:1054/5960 train_time:1014556ms step_avg:962.58ms
step:1055/5960 train_time:1015519ms step_avg:962.58ms
step:1056/5960 train_time:1016485ms step_avg:962.58ms
step:1057/5960 train_time:1017447ms step_avg:962.58ms
step:1058/5960 train_time:1018417ms step_avg:962.59ms
step:1059/5960 train_time:1019385ms step_avg:962.59ms
step:1060/5960 train_time:1020349ms step_avg:962.59ms
step:1061/5960 train_time:1021327ms step_avg:962.61ms
step:1062/5960 train_time:1022286ms step_avg:962.60ms
step:1063/5960 train_time:1023256ms step_avg:962.61ms
step:1064/5960 train_time:1024224ms step_avg:962.62ms
step:1065/5960 train_time:1025189ms step_avg:962.62ms
step:1066/5960 train_time:1026156ms step_avg:962.62ms
step:1067/5960 train_time:1027116ms step_avg:962.62ms
step:1068/5960 train_time:1028084ms step_avg:962.63ms
step:1069/5960 train_time:1029050ms step_avg:962.63ms
step:1070/5960 train_time:1030026ms step_avg:962.64ms
step:1071/5960 train_time:1030992ms step_avg:962.64ms
step:1072/5960 train_time:1031950ms step_avg:962.64ms
step:1073/5960 train_time:1032918ms step_avg:962.65ms
step:1074/5960 train_time:1033882ms step_avg:962.65ms
step:1075/5960 train_time:1034853ms step_avg:962.65ms
