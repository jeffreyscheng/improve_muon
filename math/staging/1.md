# Chapter X Fisher‑Whitened Gradients, Muon & Friends  
*A practitioner‑ready tour from first principles to momentum, curvature variance and implementation tricks*

---

## 1 Why Orthogonalise a Gradient?  

Training speed is limited by *curvature*: steep directions force tiny steps, flat directions waste progress.  
Full second‑order methods whiten the gradient with $H^{-1/2}$ (Newton) or the Fisher information (natural gradient), but computing either is prohibitive for modern models.

For a single $p\times p$ weight matrix we adopt the **empirical‑Fisher surrogate**
$$
H \;\approx\;G\,G^{\!\top}.
$$
Surprisingly—see Claim 3.1—this implies
$$
H^{-1/2}G
 \;=\;\operatorname{Ortho}(G)
 \;=\;\text{“the orthogonal factor of }G.”
$$
One SVD per minibatch now suffices for an *almost* second‑order step.

### Why *average ‑ then orthogonalise*?  

Noise shrinks as $k^{-1/2}$ when you average $k$ gradients.  
Applying $\operatorname{Ortho}$ *after* this linear operation lets curvature act on a much cleaner signal, so the resulting bias is $\sim1/k$.  
If you normalise first (SoupyMuon) you lock in curvature shrinkage for every sample; later averaging cannot remove it.  
Section 5 quantifies the difference.

---

## 2 Notation  

| symbol | meaning |
|--------|---------|
| $G$ | true gradient ($p\times p$) |
| $g_i=G+\varepsilon_i$ | per‑example gradient |
| $\Sigma=\operatorname{Cov}[\operatorname{vec}\varepsilon_i]$ | noise covariance |
| $H$ | empirical Fisher $GG^{\!\top}$ |
| $\tilde G$ | whitened gradient $H^{-1/2}G$ |
| $\operatorname{Ortho}(A)$ | $A\bigl(A^{\!\top}A\bigr)^{-1/2}$ |
| $k$ | minibatch size |
| $\gamma$ | momentum coefficient |

---

## 3 Two Estimators Before We Begin  

| Estimator | Construction | Steel‑man argument |
|-----------|--------------|--------------------|
| **Muon** | $\displaystyle \hat G_{\text{Muon}}=\operatorname{Ortho}\!\Bigl(\tfrac1k\sum_i g_i\Bigr)$ | Averaging removes noise *before* the expensive non‑linear map. |
| **SoupyMuon** | $\displaystyle \hat G_{\text{Soupy}}=\tfrac1k\sum_i \operatorname{Ortho}(g_i)$ | Normalising each $g_i$ discards scale information immediately, plausibly yielding purer directional data. |

The rest of the chapter determines which intuition survives rigorous bias–variance analysis.

---

## 4 Bedrock: Unbiasedness of SGD (Claim 1)  

For completeness:

$$\begin{aligned}
g(\theta;B) &=\tfrac1{|B|}\sum_{x\in B}\nabla_\theta\ell(\theta;x),\\
\mathbb E_B[g(\theta;B)] &=\nabla_\theta L(\theta).
\end{aligned}$$

Everything that follows (momentum, Muon, etc.) builds on this fact.

---

## 5 The Orthogonal Factor Equals Fisher Whitening (Claim 3.1 = Sibling 2)  

With the thin SVD $G=USV^{\!\top}$,

$$\begin{aligned}
(GG^{\!\top})^{-1/2}G &= U S^{-1}U^{\!\top}USV^{\!\top}=UV^{\!\top},\\
\operatorname{Ortho}(G) &= G(G^{\!\top}G)^{-1/2}=USV^{\!\top}VS^{-1}V^{\!\top}=UV^{\!\top}.
\end{aligned}$$

Hence $\tilde G=\operatorname{Ortho}(G)$.

---

## 6 Rank‑1 Integrability vs. Matrix Non‑Integrability (Sibling 3 & 4)  

*Vector case ($m=1$).*    
$\operatorname{Ortho}(g)=g/\|g\|=\nabla_g\|g\|$—it is the gradient of a scalar potential.

*Matrix case ($m\ge2$).*    
Cross‑derivatives of $\operatorname{Ortho}$ fail to commute (skew part $\pm\frac12$ on elementary directions), so **no scalar field integrates to $\operatorname{Ortho}(G)$.**  
You cannot “move Muon inside the computational graph” by pre‑ or post‑multiplying with scalars.

---

## 7 Perturbation Lemma — The Shared Work‑Horse  

### 7.1 Why we linearise  

Bias and variance of Muon/SoupyMuon depend on how $\operatorname{Ortho}$ responds to a *small* noise matrix $E$.  
A first‑order expansion supplies that response once and for all.

### 7.2 Lemma (First‑order expansion)  

Define  

$$
O=\operatorname{Ortho}(G),\quad
M=G^{\!\top}G,\quad
P_{\!\perp}(E)=E-\tfrac12\,O\bigl(O^{\!\top}E+E^{\!\top}O\bigr).
$$

Then

$$
\operatorname{Ortho}(G+E)
 = \underbrace{O}_{\text{base point}}
 +\underbrace{P_{\!\perp}(E)M^{-1/2}}_{\text{tangent motion}}
 -\underbrace{\tfrac12\,O\,
   \bigl(M^{-1/2}E\bigr)^{\!\top}\!
   \bigl(M^{-1/2}E\bigr)}_{\text{radial shrinkage}}
 +\mathcal O(\lVert E\rVert^{3}).
$$

*Sketch of proof.*

1. **Gram matrix after perturbation**:  
   $(G+E)^{\!\top}(G+E)=M+D+E^{\!\top}E$ with $D=G^{\!\top}E+E^{\top}G$.
2. **Need its $-1/2$ power** for $\operatorname{Ortho}$, hence expand $(M+D)^{-1/2}$ via the binomial series.
3. **Plug back** into $(G+E)(\dots)^{-1/2}$—this is *exactly* the definition of $\operatorname{Ortho}(G+E)$—and collect linear vs. quadratic pieces.

---

## 8 Bias–Variance Analysis via the Lemma  

Write
$$
\gamma=\frac{\operatorname{tr}\Sigma}{k\,\lVert G\rVert_F^{2}},\qquad
\Pi_{\text{tan}}=\text{projector onto the tangent plane at }O.
$$

### 8.1 Muon  

$$\begin{aligned}
\mathbb E[\hat G_{\text{Muon}}]-\tilde G
  &= -\tfrac12\gamma\,P_{\!\perp}(G)+\mathcal O(k^{-2}),\\
\operatorname{Var}[\operatorname{vec}\hat G_{\text{Muon}}]
  &= \tfrac{\operatorname{tr}\Sigma}{k\,\lVert G\rVert_F^{2}}\,
     \Pi_{\text{tan}}+\mathcal O(k^{-3/2}).
\end{aligned}$$

**Interpretation.**  Averaging first yields the extra $1/k$ in bias.

### 8.2 SoupyMuon  

$$\begin{aligned}
\mathbb E[\hat G_{\text{Soupy}}]-\tilde G
  &= -\tfrac12
     \frac{\operatorname{tr}\Sigma}{\lVert G\rVert_F^{3}}\,
     P_{\!\perp}(G)+\mathcal O(\sigma^{3}),\\
\operatorname{Var}&= \text{same leading term as Muon}.
\end{aligned}$$

Bias no longer scales with $k$.

### 8.3 Debiased‑Muon  

Estimate $\gamma$ with the sample covariance and divide $\operatorname{Ortho}(\bar g)$ by $(1-\tfrac12\hat\gamma)$; bias shrinks to $\mathcal O(k^{-2})$ while variance remains $O(1/k)$.

---

## 9 Momentum: Bias, Variance & When It Helps (Sibling 5 & 6)  

Let  
$m_{t}=\gamma m_{t-1}+(1-\gamma)\,\hat g_{t}$  
with stationary mean $μ$ and variance $σ^{2}$ of $\hat g_t$.

$$\begin{aligned}
\text{Bias}(m_t)      &= -μ\,\gamma^{t},\\
\Var(m_t)             &= σ^{2}\,\tfrac{1-\gamma}{1+\gamma}\bigl(1-\gamma^{2t}\bigr),\\
\mathrm{MSE}(m_t)     &= μ^{2}\gamma^{2t}+σ^{2}\tfrac{1-\gamma}{1+\gamma}(1-\gamma^{2t}).
\end{aligned}$$

**Warm‑up length** (time until momentum beats raw SGD):

$$
t > T^{*}
   = \frac{\ln\!\bigl[\tfrac{2\gamma}{(1+\gamma)\frac{μ^{2}}{σ^{2}}-(1-\gamma)}\bigr]}
          {2\ln\gamma}.
$$

Low SNR ($μ^{2}\!\ll\!σ^{2}$) ⇒ large $\gamma$ is beneficial immediately.

*Momentum + Muon.*  
Muon’s $1/k$ bias multiplies $\gamma^{t}$, so a gentle warm‑up (smaller $\gamma$ in first few steps) avoids compounding early curvature bias.

---

## 10 Further Results & Implementation Nuggets  

* **Asymptotics of Ortho‑of‑Sum vs. Sum‑of‑Ortho (Sibling 8 & 9).**  
  Ortho‑after‑sum incurs $O(k\,\Sigma)$ bias when noise is anisotropic; sum‑of‑Ortho stays $O(\Sigma)$.  Explains occasional wins of Soupy‑style updates for extremely skewed noise.

* **Riemannian curvature variance $\sigma_R$ (Sibling 10).**  
  $$\sigma_R^2=\frac1k\sum_i\bigl\|\log(\bar H^{-1/2}H_i\bar H^{-1/2})\bigr\|_F^{2}$$  
  where $\bar H$ is the Fréchet mean.  Track it to know when a single global whitening matrix is adequate.

* **Higham polar iteration (Sibling 11).**  
  A quadratically convergent, fp16‑stable alternative to Newton–Schulz for computing $\operatorname{Ortho}$.

* **Global whitening transform (Sibling 12).**  
  Re‑parameterise weights by $\phi=M^{1/2}\theta$ with periodically updated $M^{1/2}$ to make Muon almost free.

* **Fisher = expected Hessian for NLL (Sibling 7) and “no special name” for the general expected Hessian (Sibling 14).**  Useful lore for reading natural‑gradient papers.

---

## 11 Summary Table  

| Estimator | Bias | Variance | Extra cost | Practical verdict |
|-----------|------|----------|------------|-------------------|
| Muon | $\mathcal O(k^{-1})$ | $\mathcal O(k^{-1})$ | 1 SVD | Excellent default |
| SoupyMuon | $\mathcal O(1)$ | $\mathcal O(k^{-1})$ | $k$ SVDs | Use only for extreme anisotropy |
| Debiased‑Muon | $\mathcal O(k^{-2})$ | $\mathcal O(k^{-1})$ | Muon + 1 trace | Best bias if you can spare the trace |

---

## 12 Practical Take‑Aways  

* **Average → Ortho is principled.**  Linearity kills half the noise; curvature then shrinks bias by an extra $1/k$.  
* **Momentum helps when SNR is low** but biases early steps—ramp $\gamma$ gradually if using Muon.  
* **Track $\sigma_R$.**  A stable curvature spectrum means global whitening works; spikes call for richer preconditioners.  
* **Implementation cheat‑sheet.**  
  * One SVD per minibatch → Muon.  
  * Add one trace → Debiased‑Muon for $k^{-2}$ bias.  
  * Swap Newton–Schulz for Higham in fp16.  
  * Consider a global whitening change‑of‑variables to amortise SVDs.  
* **Scalar/vector parameters are easy.**  Normalise the gradient; Muon collapses to $g/\|g\|$.  
* **Matrices need Muon‑style machinery.**  No scalar potential gives $\operatorname{Ortho}(G)$ for $m\ge2$.

Armed with these results, you can pick the right preconditioned gradient, momentum schedule and curvature tracker to squeeze maximum performance from large‑scale models—without ever inverting a full Fisher matrix.
