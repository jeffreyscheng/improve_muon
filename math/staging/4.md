# Biasâ€“Variance Geometry of Pre-conditioned Gradients  

These notes consolidate our derivations on how minibatch size, momentum, and Richardson extrapolation shape bias & variance when estimating the pre-conditioned gradient $H^{-1/2}G$ for a parameter block.

---

## Notation  

| symbol | definition |
|--------|------------|
| $\ell(\theta,x)$ | per-token loss |
| $L(\theta)=\mathbb E_{x}[\ell(\theta,x)]$ | full loss |
| $G=\nabla_\theta L(\theta)$ | population gradient |
| $g_i$ | micro-batch gradient |
| $H=\nabla_\theta^2L(\theta)=G\,G^{T}$ | *assumption* |
| $\operatorname{Ortho}(A)$ | polar unitary factor of $A$ |
| $n$ | tokens in an averaged gradient |
| $\delta_i=g_i-G$ | zero-mean noise |

---

## Claims & Justifications  

### **Claim 1**  
$H^{-1/2}G$ equals the polar unitary factor $\operatorname{Ortho}(G)$ (under $H=G\,G^{T}$).

**Justification**  
$$\begin{aligned}
\text{SVD: } & G = U\,S\,V^{T} \\[4pt]
H &= G\,G^{T} = U\,S\,V^{T}V\,S\,U^{T} = U\,S^{2}U^{T} \\[4pt]
H^{-1/2} &= U\,S^{-1}U^{T} \\[4pt]
H^{-1/2}G &= U\,S^{-1}U^{T}\,U\,S\,V^{T} = U\,V^{T} = \operatorname{Ortho}(G).
\end{aligned}$$  

---

### **Claim 2**  
For a minibatch of $n$ tokens,  
$$\mathbb E[g_n]=G+\dfrac{b}{n}+O(n^{-2}),$$  
with a *single* vector $b$ independent of $n$.

**Justification**  
$$\begin{aligned}
E_n &= \frac1n\sum_{i=1}^{n}\delta_i,\qquad \mathbb E[E_n]=0, \; \operatorname{Cov}(E_n)=\frac{\Sigma}{n}.\\[4pt]
\operatorname{Ortho}(G+E_n) &= \operatorname{Ortho}(G) + D_G[E_n] + Q_G[E_n,E_n] + O(\|E_n\|^{3}).\\[4pt]
\mathbb E[D_G[E_n]] &= 0 \quad (\text{linearity + zero mean}).\\[4pt]
\mathbb E[Q_G[E_n,E_n]] &= \frac1n\,T(\Sigma) \quad (\text{quadratic $\propto$ covariance}).\\[4pt]
\text{Define } & b:=T(\Sigma). \quad \square
\end{aligned}$$  

---

### **Claim 3**  
$\bigl\{\mathbb E[g_{k/8}],\mathbb E[g_{k/4}],\dots,G\bigr\}$ are collinear.

**Justification**  
$$\begin{aligned}
\mathbb E[g_{n_1}] - \mathbb E[g_{n_2}]
  &= b\!\left(\frac1{n_1}-\frac1{n_2}\right)
  = \lambda\,b,\;\lambda\in\mathbb R.
\end{aligned}$$  

---

### **Claim 4**  
For any quadratic norm $\|x\|_{Q}^{2}=x^{T}Qx$ ($Q\succ0$):  
$$\text{MSE} = \|\mathbb E[\hat g]-G\|_{Q}^{2} + \operatorname{tr}\!\bigl(Q\,\operatorname{Cov}(\hat g)\bigr).$$  

**Justification**  
$$\begin{aligned}
\operatorname{MSE} &= \mathbb E\|\,\hat g-G\|_{Q}^{2} \\[4pt]
&= \mathbb E\|\,\hat g-\mathbb E[\hat g] + \mathbb E[\hat g]-G\|_{Q}^{2} \\[4pt]
&= \underbrace{\mathbb E\|\,\hat g-\mathbb E[\hat g]\|_{Q}^{2}}_{\text{variance}}
  + \underbrace{\|\mathbb E[\hat g]-G\|_{Q}^{2}}_{\text{bias}^{2}}.
\end{aligned}$$  

---

### **Claim 5**  
A momentum EMA adds a *staleness* bias  
$\|\Delta_{\text{stale}}\| = O\!\bigl(\eta\,\tfrac{\gamma}{1-\gamma}\,\|H^{1/2}G\|\bigr)$.

**Justification**  
$$\begin{aligned}
\hat g^{(t)} &= \sum_{j=0}^{\infty}\alpha_j\,G(\theta^{t-j}),\;
\alpha_j=(1-\gamma)\gamma^j.\\[4pt]
G(\theta^{t-j}) &\approx G(\theta^{t}) + H(\theta^{t})(\theta^{t-j}-\theta^{t}).\\[4pt]
\theta^{t-j}-\theta^{t} &\approx -j\,\eta\,H^{-1/2}G.\\[4pt]
\Delta_{\text{stale}}
 &= -\eta\,H^{1/2}G\,\sum_{j\ge 0}\alpha_j j
 = -\eta\,H^{1/2}G\,\frac{\gamma}{1-\gamma}.
\end{aligned}$$  

---

### **Claim 6**  
EMA rescales minibatch noise:  
$$\operatorname{Cov}(E_M)=\frac{1-\gamma}{1+\gamma}\,\frac{\Sigma}{k}.$$  

**Justification**  
$$\begin{aligned}
E_M &= \sum_{j=0}^{\infty}\alpha_j\,\delta_{t-j},\quad \alpha_j=(1-\gamma)\gamma^{j}.\\[4pt]
\operatorname{Cov}(E_M)
 &= (1-\gamma)^{2}\sum_{j\ge 0}\gamma^{2j}\,\frac{\Sigma}{k}\\[4pt]
 &= \frac{(1-\gamma)^{2}}{1-\gamma^{2}}\,\frac{\Sigma}{k}
  = \frac{1-\gamma}{1+\gamma}\,\frac{\Sigma}{k}.
\end{aligned}$$  

---

### **Claim 7**  
Richardson extrapolation $R=2g_{2k}-g_k$ removes the $1/k$ bias but doubles variance.

**Justification**  
$$\begin{aligned}
\mathbb E[R] &= 2\Bigl(G+\tfrac{b}{2k}\Bigr) - \Bigl(G+\tfrac{b}{k}\Bigr)
            = G + O(k^{-2}).\\[6pt]
\operatorname{Var}(R)
 &= 4\,\frac{\sigma^{2}}{2k} + \frac{\sigma^{2}}{k} - 4\,\frac{\sigma^{2}}{2k}
  = \frac{\sigma^{2}}{k}
  = 2\,\operatorname{Var}(g_{2k}).
\end{aligned}$$  

---

### **Claim 8**  
With eight micro-batch gradients ($n_0=k/8$ tokens each), an unbiased variance estimator is  
$$\widehat{\sigma}^{2}=\frac{n_0}{2\binom{8}{2}}\sum_{i<j}\|g_i - g_j\|_{F}^{2}.$$

**Justification**  
$$\begin{aligned}
\mathbb E\|g_i-g_j\|_{F}^{2}
 &= \mathbb E\|\delta_i-\delta_j\|_{F}^{2} \\[4pt]
 &= \mathbb E\|\delta_i\|_{F}^{2} + \mathbb E\|\delta_j\|_{F}^{2}
    - 2\,\mathbb E\langle\delta_i,\delta_j\rangle_{F}\\[4pt]
 &= 2\,\operatorname{tr}\Bigl(\tfrac{\Sigma}{n_0}\Bigr)
  = \frac{2\sigma^{2}}{n_0}.
\end{aligned}$$  

---

### **Claim 9**  
Plotting each subset mean error against $|1/n-1/k|$ gives a line whose slope is $\|b\|$.

**Justification**  
$$\begin{aligned}
y &= \|g_n - g_{k}\|_{F},\quad
x = \Bigl|\frac1n-\frac1k\Bigr|.\\[4pt]
\mathbb E[y] &= \|\,\mathbb E[g_n] - \mathbb E[g_k]\,\|_{F}
             = \|b\|_{F}\,x.
\end{aligned}$$  

---

### **Claim 10**  
Leave-one-out jack-knife already returns $\|b\|/k$; multi-$k$ regression mainly tightens the slope estimate and checks the $1/n$ law.

**Justification**  
$$\begin{aligned}
\text{Jack-knife bias } 
 &= (m-1)\bigl(\overline{g_{(7)}}-g_8\bigr),\;m=8 \\[4pt]
\mathbb E[\overline{g_{(7)}}] 
 &= G + \frac{8b}{7k},\;\;
\mathbb E[g_8] = G + \frac{b}{k}.\\[4pt]
\Rightarrow\; \mathbb E[\text{Jack-knife bias}] 
 &= 7\Bigl(\frac{8b}{7k}-\frac{b}{k}\Bigr)
 = \frac{b}{k}.
\end{aligned}$$  

---