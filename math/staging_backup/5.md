# Orthogonalised-Gradient Analysis

**Motivation**  
Large-matrix LLM weights suffer from ill-conditioned curvature.  Pre-conditioning the raw gradient
$$\widetilde G := H^{-1/2} G$$
“whitens’’ directions, giving stable, direction-agnostic learning-rates.  Under the **factorised curvature assumption**
$$H = GG^{\top},$$
one thin SVD of the gradient makes the square-root free.  Two estimators naturally follow:

* **Uon** – orthogonalise a single mini-batch mean.  
* **Muon** – orthogonalise an exponential moving average (EMA) with momentum parameter $\gamma$.

---

## **Claim 1**

$\displaystyle \widetilde G = \operatorname{Ortho}(G)$.

**Justification**

$$\begin{aligned}
&\text{Let } G = U S V^{\top},
      \quad U,V \in \mathbb{R}^{p \times r},
      \quad S = \operatorname{diag}(s_1,\dots,s_r) \succ 0.\\
&H = G G^{\top}
     = (U S V^{\top})(V S U^{\top})
     = U S^{2} U^{\top}.\\
&H^{-\frac{1}{2}}
     = U S^{-1} U^{\top}
     \quad(\text{because } S^{-1} S^{2} S^{-1} = I).\\
&\widetilde G
     = H^{-\frac{1}{2}} G
     = U S^{-1} U^{\top}\,(U S V^{\top})
     = U V^{\top}
     = \operatorname{Ortho}(G).\;\square
\end{aligned}$$

---

## **Claim 2**

With i.i.d. per-token gradients $g_x$ satisfying  
$\mathbb{E}[g_x]=G$ and $\operatorname{Cov}(g_x)=\Sigma$,

*Uon*  
$$U_{\text{on}}
   := \operatorname{Ortho}\!\Bigl(\widehat G\Bigr),
   \quad
   \widehat G := \frac{1}{k}\sum_{i=1}^{k} g_i$$

has  

* bias  
  $$\mathbb{E}[U_{\text{on}}]-\widetilde G
        = -\frac{1}{2k}\;
          P_G \Sigma P_G^{\top}\,V\,S^{-2}
          + O\!\bigl(k^{-2}\bigr),$$
* covariance  
  $$\operatorname{Cov}(U_{\text{on}})
        = \frac{1}{k}\;
          P_G \Sigma P_G^{\top}
          + O\!\bigl(k^{-2}\bigr).$$

**Justification**

$$\begin{aligned}
&\text{(1)  First-order Fréchet derivative of Ortho at }G:\\
&\qquad\operatorname{D\,Ortho}[G](E)
    = P_G\,E\,V\,S^{-1},\\
&\text{where } P_G := U U^{\top}.\\[6pt]
&\text{(2)  Second-order term (Higham 2008, §6.6):}\\
&\qquad\operatorname{D}^2\!\operatorname{Ortho}[G](E,E)
    = -P_G\,E\,V\,S^{-2} E^{\top} P_G^{\top}.\\[6pt]
&\text{(3)  Put } E := \widehat G - G
      = \frac{1}{k}\sum_{i=1}^{k}(g_i - G). \\
&\qquad\mathbb{E}[E]=0,\;
      \mathbb{E}[E E^{\top}] = \frac{1}{k}\Sigma.\\[6pt]
&\text{(4)  Expectation of the Taylor series:}\\
&\qquad\mathbb{E}[U_{\text{on}}]
      = \widetilde G
        + \frac{1}{2}\,
          \mathbb{E}\bigl[\operatorname{D}^2\!\operatorname{Ortho}[G](E,E)\bigr]
        + O(k^{-2}).\\
&\qquad\Rightarrow \text{Bias } = -\frac{1}{2k}\,
          P_G \Sigma P_G^{\top}\,V\,S^{-2}
          + O(k^{-2}).\\[6pt]
&\text{(5)  Covariance comes from the first-order term:}\\
&\qquad\operatorname{Cov}(U_{\text{on}})
      = P_G\,\mathbb{E}[E E^{\top}]\,P_G^{\top}
        + O(k^{-2})
      = \frac{1}{k}\,P_G\Sigma P_G^{\top}
        + O(k^{-2}).\;\square
\end{aligned}$$

---

## **Claim 3**

For momentum $\gamma$ define the EMA gradient  
$$\widehat G^{(t)}
     := (1-\gamma)\sum_{s=0}^{\infty} \gamma^{s}\,
        \overline g^{(t-s)},
\quad
\overline g^{(t)} := \frac{1}{k}\sum_{i=1}^{k} g_i^{(t)},$$
and set
$$M_{\text{on}} := \operatorname{Ortho}\!\bigl(\widehat G^{(t)}\bigr).$$

Then  

* bias matches Uon to $O(k^{-2})$,  
* covariance shrinks by $\dfrac{1-\gamma}{1+\gamma}$:

$$\operatorname{Cov}(M_{\text{on}})
      = \frac{1-\gamma}{1+\gamma}\;
        \frac{1}{k}\;
        P_G \Sigma P_G^{\top}
        + O\!\bigl(k^{-2}\bigr).$$

**Justification**

$$\begin{aligned}
&\text{(1)  EMA is a linear time-invariant filter.}\;\\
&\qquad\operatorname{Cov}\!\bigl(\widehat G^{(t)}\bigr)
     = \frac{1-\gamma}{1+\gamma}\,
       \frac{\Sigma}{k}.\\[6pt]
&\text{(2)  Insert this }E := \widehat G^{(t)} - G
    \text{ into the same second-order expansion as Claim 2.}\\
&\text{(3)  Result: identical bias term, but covariance multiplied by }
    \frac{1-\gamma}{1+\gamma}.\;\square
\end{aligned}$$

---

## **Claim 4**

For two **independent** Muon estimates with batch sizes $B_1,B_2$  
$$\mathbb{E}\Bigl[\|M_{B_1}-M_{B_2}\|_{F}^{2}\Bigr]
    = \frac{1-\gamma}{1+\gamma}\,
      \Bigl(\frac{1}{B_1}+\frac{1}{B_2}\Bigr)\,
      \operatorname{tr}(P_G\Sigma)
      + O\!\bigl(B_1^{-2}+B_2^{-2}\bigr).$$

**Justification**

$$\begin{aligned}
&M_{B_j}
   = \widetilde G + \frac{b}{B_j} + \eta_{B_j},\\
&\mathbb{E}[\eta_{B_j}] = 0,\;
  \operatorname{Cov}(\eta_{B_j})
     = \frac{1-\gamma}{1+\gamma}\,
       \frac{\Sigma}{B_j}.\\[6pt]
&\|M_{B_1}-M_{B_2}\|_{F}^{2}
   = \|\eta_{B_1}-\eta_{B_2}\|_{F}^{2}
     + O\!\bigl(B_1^{-2}+B_2^{-2}\bigr).\\
&\mathbb{E}[\|\eta_{B_1}-\eta_{B_2}\|_{F}^{2}]
   = \operatorname{tr}\!\bigl(
        \operatorname{Cov}(\eta_{B_1})
        + \operatorname{Cov}(\eta_{B_2})
     \bigr).\\
&\text{Insert the covariances to obtain the stated formula}.\;\square
\end{aligned}$$

---

## **Claim 5**

When the smaller batch ($B_1=ik$) is a **subset** of the larger batch ($B_2=8k$):

$$\mathbb{E}\bigl[\|M_{ik}-M_{8k}\|_{F}\bigr]
   = C\,
     \sqrt{\frac{1}{ik}-\frac{1}{8k}}
     + O\!\bigl(k^{-3/2}\bigr),\quad
   C := \sqrt{\frac{1-\gamma}{1+\gamma}\,
              \operatorname{tr}\Sigma}.$$

**Justification**

$$\begin{aligned}
&\operatorname{Cov}(\eta_{ik},\eta_{8k})
     = \operatorname{Cov}(\eta_{ik})
     = \frac{1-\gamma}{1+\gamma}\,
       \frac{\Sigma}{ik}.\\
&\operatorname{Var}\!\bigl(M_{ik}-M_{8k}\bigr)
   = \frac{1-\gamma}{1+\gamma}\,
     \Sigma\Bigl(
       \frac{1}{8k}-\frac{1}{ik}
     \Bigr).\\
&\text{Trace then square-root gives the expectation of the Frobenius norm}.\;\square
\end{aligned}$$

---

## **Claim 6**

**Richardson extrapolation**

$$M^{\mathrm{RE}}_{B,c}
    := \frac{c\,M_{cB}-M_B}{c-1},
    \qquad c>1$$

* cancels the $O(B^{-1})$ bias,  
* inflates covariance by  
  $$\frac{c+1}{(c-1)^{2}}\;.$$

**Justification**

$$\begin{aligned}
&M_{B,c}^{\mathrm{RE}}
   = \frac{c}{c-1}
     \bigl(\widetilde G + \tfrac{b}{cB} + \eta_{cB}\bigr)
     -\frac{1}{c-1}
     \bigl(\widetilde G + \tfrac{b}{B} + \eta_B\bigr)\\
&\phantom{M_{B,c}^{\mathrm{RE}}}
   = \widetilde G
     + \frac{c\,b-b}{(c-1)cB}
     + \frac{c\,\eta_{cB}-\eta_B}{c-1}.\\
&\text{Bias term }c\,b-b = 0
   \;\Rightarrow\; \text{leading bias }O(B^{-2}).\\
&\operatorname{Cov}\bigl(M_{B,c}^{\mathrm{RE}}\bigr)
   = \frac{c^{2}\operatorname{Cov}(\eta_{cB})
          + \operatorname{Cov}(\eta_B)}{(c-1)^{2}}
   = \frac{c+1}{(c-1)^{2}}\,
     \frac{1-\gamma}{1+\gamma}\,
     \frac{\Sigma}{B}.\;\square
\end{aligned}$$

---

## **Claim 7**

Methods that **cut variance while accepting extra bias**:

| Technique | Variance Mechanism | Bias Origin |
|-----------|-------------------|-------------|
| Gradient / value clipping | Truncates outliers | Non-linear truncation shifts the mean |
| Sign-SGD or top-$k$ sparsification | Discards mantissa or small coordinates | Non-linear compression map |
| Low-precision or stochastic rounding | Randomly cancels round-off | Mean of round-off $\neq 0$ |
| Error-feedback clipping | Feedback halves clip noise | Feedback decay leaves residual bias |
| Polyak parameter averaging | Time average $\propto 1/T$ variance | Early iterates overweighted |

**Justification**  
Deriving each bias term uses the same second-order Fréchet expansion strategy as Claims 2–3, replacing  
$\operatorname{Ortho}$ by the non-linear map corresponding to the technique.  Complete proofs are omitted for brevity.  
**Unjustified Conjecture** – full algebra available upon request.

---
