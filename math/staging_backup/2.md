# Bias vs. Variance in “Orthogonalised” Stochastic Gradients  

## Motivation  
Large-scale optimisers often pre-condition a per-sample gradient  
$g=\nabla_\theta \ell(\theta,x)$ by a (square-root) inverse Hessian such as  
$H^{-\tfrac12}g$ with $H\approx gg^{\!\top}$.  
To reduce communication one may keep **only the direction**

* rank-1 case $\operatorname{Ortho}(g)=\dfrac{g}{\lVert g\rVert}$  
* rank-$r$  $\operatorname{Ortho}(G)=U_G V_G^{\!\top}$ for $G$’s thin SVD  

and average these directional objects across workers.  
We quantify the **bias** (systematic offset) and **variance** (random spread) introduced by that choice so you can decide whether to spend hardware on larger all-gathers (bias reduction) or on model-soup / EMA (variance reduction).

---

## **Claim 1** — *Normalise after* averaging is asymptotically unbiased  

**Justification**  
$$
\begin{aligned}
g_i &= g+\varepsilon_i,
&\mathbb{E}[\varepsilon_i]&=0,
&\operatorname{Cov}(\varepsilon_i)&=\Sigma,\\
G   &=\lVert g\rVert,
&z &=\dfrac{g}{G},
&P_\perp&=I-zz^{\!\top},\\[6pt]
\bar g&=\frac1n\sum_{i=1}^{n}g_i
      =g+\bar\varepsilon,
&\operatorname{Cov}(\bar\varepsilon)&=\frac{\Sigma}{n}.
\end{aligned}
$$

Second-order Taylor of $f(v)=\dfrac{v}{\lVert v\rVert}$ around $v=g$:  
$$
\begin{aligned}
f(g+\delta)
&= z+\frac1G P_\perp\delta
   -\frac1{2G^{3}}\,
     z\,\delta^{\!\top}P_\perp\delta
   +O(\lVert\delta\rVert^{3}).\\[6pt]
\mathbb{E}[f(\bar g)]
&= z
   -\frac{1}{2G^{3}}\,
     z\,\mathbb{E}\!\bigl[\bar\varepsilon^{\!\top}P_\perp\bar\varepsilon\bigr]
   +O(n^{-\tfrac32})\\
&= z
   -\frac{\operatorname{tr}(P_\perp\Sigma)}{2G^{3}n}\,z
   +O(n^{-\tfrac32}).
\end{aligned}
$$  
Hence $\bigl\lVert\mathbb{E}[f(\bar g)]-z\bigr\rVert=O\!\bigl(\tfrac1n\bigr)$.

---

## **Claim 2** — *Normalise before* averaging retains an $O(1)$ bias  

**Justification**  
Apply the same expansion to each $g_i$:
$$
\begin{aligned}
\mathbb{E}\!\bigl[f(g_i)\bigr]
&= z
   -\frac{\operatorname{tr}(P_\perp\Sigma)}{2G^{3}}\,z
   +O(\lVert\Sigma\rVert^{2}).\\
\mathbb{E}\!\Bigl[\tfrac1n\textstyle\sum_{i}f(g_i)\Bigr]
&= z
   -\frac{\operatorname{tr}(P_\perp\Sigma)}{2G^{3}}\,z
   +O(\lVert\Sigma\rVert^{2}),
\end{aligned}
$$
so the bias **does not decay with** $n$.

---

## **Claim 3** — For rank $r\ge2$ a single scalar cannot remove the bias  

**Justification**  
Let $G=USV^{\!\top}$ with $S=\operatorname{diag}(s_1,\dots,s_r)$.  
A perturbation $\Delta$ gives (Fréchet derivatives, full expansion):
$$
\begin{aligned}
\operatorname{Ortho}(G+\Delta)
&=  UV^{\!\top}
   +P_\perp\Delta V^{\!\top}
   +U\Delta^{\!\top}P_\perp V^{\!\top}\\
&\quad
   -U
    \Bigl(
      U^{\!\top}\Delta P_\perp\Delta^{\!\top}U
      +V^{\!\top}\Delta^{\!\top}P_\perp\Delta V
     \Bigr)
     V^{\!\top}
     S^{-2}
   +O(\lVert\Delta\rVert^{3}).
\end{aligned}
$$
The expectation of the linear terms over noise produces a *constant*  
$B\in U^{\perp}$.  Multiplying the averaged matrix by any scalar $\alpha$
rescales $UV^{\!\top}$ **and** $B$ but cannot move $B$ to $0$ without
destroying $UV^{\!\top}$.

---

## **Claim 4** — Micro-batch of size $m$, averaged over $n$  

**Justification**  
For one micro-batch  
$\bar G_k=\dfrac1m\sum_{i=1}^{m}g_{k,i}=G+\Delta_k$ with
$\operatorname{Cov}(\Delta_k)=\dfrac{\Sigma}{m}$.  
Insert $\Delta=\Delta_k$ in the expansion above and take the mean:
$$
\begin{aligned}
\mathbb{E}[W_k]
&= UV^{\!\top}
   -\frac{1}{m}\,U\Gamma V^{\!\top}
   +O(m^{-\tfrac32}),\\
\operatorname{Bias}(\widehat Z)
&= -\frac{1}{m}\,U\Gamma V^{\!\top}
   +O(m^{-\tfrac32}),\\
\operatorname{Var}(\widehat Z)
&= \frac{1}{nm}\,{\cal J}\Sigma{\cal J}^{\!\top}
   +O\bigl((nm)^{-\tfrac32}\bigr).
\end{aligned}
$$
Thus **bias $\propto 1/m$** while **variance $\propto 1/(nm)$**.

---

## **Claim 5** — Double orthogonalisation keeps $O(1)$ bias  

**Justification**  
Let $B$ be the $O(1)$ bias of each single $W_i$.  
Jacobian action: ${\cal J}[B]=-B_\perp$.  
Quadratic terms contribute $-B/n$.  Therefore
$$
\mathbb{E}\bigl[\operatorname{Ortho}(\bar W)\bigr]
  = UV^{\!\top}+B_\perp-\tfrac1n B + O(\Sigma^{2}),
$$
and an $O(1)$ piece $B_\perp$ remains irrespective of $n$.

---

## **Claim 6** — Adding one scalar norm per sample restores $O(1/n)$ bias  

**Justification**  
Store $\rho_x=\lVert g_x\rVert_F$ and send
$\rho_x W_x$ instead of $W_x$.
Since $g_x=\rho_x W_x$ we have
$\mathbb{E}[\rho_x W_x]=\mathbb{E}[g_x]=G$.
Applying a **single** $\operatorname{Ortho}$ after the average reduces bias exactly as in Claim 1, i.e. $O(1/n)$.

---

## **Claim 7** — Richardson extrapolation isolates bias coefficient  

**Justification**  
Assume expansion
$\mathbb{E}[Z_{(b)}]=Z_\infty+\dfrac{c}{b}+O(b^{-2})$.  
Empirical means:
$$
\bar Z_{(m)},\qquad
\bar Z_{(2m)}.
$$
Construct  
$$
Z_{\text{Rich}}=2\,\bar Z_{(2m)}-\bar Z_{(m)},\qquad
\widehat c = 2m\,(\bar Z_{(m)}-\bar Z_{(2m)}).
$$
Then
$$
\begin{aligned}
\mathbb{E}[Z_{\text{Rich}}]
   &= 2\!\left(Z_\infty+\frac{c}{2m}\right)
      -\!\left(Z_\infty+\frac{c}{m}\right)
      = Z_\infty+O(m^{-2}),\\[4pt]
\mathbb{E}[\widehat c]
   &= 2m\!\left(\frac{c}{m}-\frac{c}{2m}\right)
      +O(m^{-1})
      = c + O(m^{-1}).
\end{aligned}
$$
Thus the leading $c/b$ term cancels, exposing both $Z_\infty$ and $c$.

---

## **Claim 8** — Bias-vs-variance diagnostic without giant batches  

**Justification**  
1. Collect $K$ raw gradients $G_i$ (already computed during training).  
2. Cheap updates $Z_i$ give  
   $$\sigma_{\text{var}}^{2}=\frac{1}{K-1}\sum_{i}
     \bigl\|Z_i-\bar Z_{(m)}\bigr\|_F^{2},\qquad
     \bar Z_{(m)}=\frac1K\sum_i Z_i.$$  
3. Form synthetic size-$2m$ averages  
   $\widetilde G_j=\tfrac12(G_{2j-1}+G_{2j}) \Rightarrow Z_j^{(2m)}$.  
4. Estimate bias magnitude  
   $$\text{bias}_{(m)}\approx
     \bigl\|\bar Z_{(m)}-\bar Z_{(2m)}\bigr\|_F.$$  
5. Signal-to-noise ratio  
   $$\text{SNR}=\dfrac{\text{bias}_{(m)}}{\sigma_{\text{var}}}$$  
   decides: low SNR ⇒ invest in variance-reduction (model-soup);  
   high SNR ⇒ invest in bigger all-gathers (bias-reduction).

---
