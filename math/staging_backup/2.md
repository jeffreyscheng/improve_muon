# Bias-vs-Variance Cheat-Sheet for “Orthogonalised” Pre-conditioned Gradients  

This note distils our conversation into a practitioner-oriented reference.
Each **Claim** is followed by  

* **Why you care** – motivation in plain English,  
* **Justification** – step-by-step algebra in  
  ```$$\begin{aligned} … \end{aligned}$$```  blocks  
  (no steps skipped; no prior knowledge of Higham required).  

If a proof does not reach that standard it is marked **Unjustified Conjecture**.

---

## **Claim 1 – Vector case:**  
Normalising **after** averaging is asymptotically unbiased.

**Why you care**  
If each GPU holds one micro-batch you can all-reduce raw gradients, divide
once by their norm, and know that the systematic error vanishes like
\(1/n\). No extra tricks required.

**Justification**  
$$\begin{aligned}
g_i &= g+\varepsilon_i,
&\mathbb E[\varepsilon_i]&=0,
&\operatorname{Cov}(\varepsilon_i)&=\Sigma,\\
G   &=\lVert g\rVert,
&z &=\dfrac{g}{G},
&P_\perp&=I-zz^{\!\top},\\
\bar g&=\frac1n\sum_{i=1}^{n}g_i
       =g+\bar\varepsilon,
&\operatorname{Cov}(\bar\varepsilon)&=\frac{\Sigma}{n}.
\end{aligned}$$

Second-order Taylor of \(f(v)=v/\lVert v\rVert\) at \(v=g\):
$$\begin{aligned}
f(g+\delta)
&=z+\frac1G P_\perp\delta
  -\frac1{2G^{3}}\,z\,\delta^{\!\top}P_\perp\delta
  +O(\lVert\delta\rVert^{3}).
\end{aligned}$$

Taking expectations:
$$\begin{aligned}
\mathbb E[f(\bar g)]
&=z
  -\frac{\operatorname{tr}(P_\perp\Sigma)}{2G^{3}n}\,z
  +O(n^{-\tfrac32}),
\end{aligned}$$
so the bias magnitude is \(O(1/n)\).

---

## **Claim 2 – Vector case:**  
Normalising **before** averaging keeps an \(O(1)\) bias.

**Why you care**  
Averaging already-normalised gradients (e.g. if you streamed them to disk)
will *never* converge to the true direction – no amount of extra SGD
steps can fix the systematic error.

**Justification**  
Apply the same expansion to every \(g_i\):
$$\begin{aligned}
\mathbb E[f(g_i)]
&=z-\frac{\operatorname{tr}(P_\perp\Sigma)}{2G^{3}}\,z
  +O(\lVert\Sigma\rVert^{2}),\\
\mathbb E\!\Bigl[\tfrac1n\sum_{i}f(g_i)\Bigr]
&=z-\frac{\operatorname{tr}(P_\perp\Sigma)}{2G^{3}}\,z
  +O(\lVert\Sigma\rVert^{2}),
\end{aligned}$$
hence bias does **not** shrink with \(n\).

---

## **Claim 3 – Rank \(r\ge2\):**  
*A single scalar re-scaling cannot remove the bias*.

**Why you care (most important)**  
Many engineers suggest “just multiply the averaged frame by a constant
to bring it back onto the Stiefel manifold”.  
This claim proves that idea cannot work once your gradient matrix has more
than one non-zero singular value (typical for LLM layers).

**Justification**  
Let  
\(G = USV^{\!\top},\;S=\operatorname{diag}(s_1,\dots,s_r)\).  
For a small perturbation \(\Delta\):

$$\begin{aligned}
\operatorname{Ortho}(G+\Delta)
&=UV^{\!\top}
  +P_\perp\Delta V^{\!\top}
  +U\Delta^{\!\top}P_\perp V^{\!\top}\\
&\quad
  -U
   \Bigl(
     U^{\!\top}\Delta P_\perp\Delta^{\!\top}U
     +V^{\!\top}\Delta^{\!\top}P_\perp\Delta V
    \Bigr)
    V^{\!\top}S^{-2}
  +O(\lVert\Delta\rVert^{3}).
\end{aligned}$$

Taking expectation over noise with
\(\operatorname{Cov}(\Delta)=\Sigma\) gives a constant
\(B\in U^{\perp}\) (first line) **orthogonal** to \(UV^{\!\top}\).
For any scalar \(\alpha\):
$$\mathbb E[\alpha(UV^{\!\top}+B)]=\alpha UV^{\!\top}+\alpha B.$$
Because \(UV^{\!\top}\) and \(B\) are in orthogonal subspaces,
no \(\alpha\) annihilates \(B\) while keeping \(UV^{\!\top}\neq0\).  
Therefore a single global scalar (or any fixed right-multiplier) cannot
remove the bias for \(r\ge2\).

---

## **Claim 4 – Micro-batch averaging (\(m\) tokens × \(n\) workers)**  

**Why you care**  
Shows the exact scaling: bias falls like \(1/m\), variance like
\(1/(nm)\).  Decide whether to enlarge *per-GPU* batches or add more
GPUs.

**Justification**  
For one micro-batch  
$$\operatorname{Cov}(\bar G_k)=\frac{\Sigma}{m}.$$  
Plug \(\Delta=\bar G_k-G\) into the expansion above:

$$\begin{aligned}
\mathbb E[W_k]
&=UV^{\!\top}-\frac1m\,U\Gamma V^{\!\top}+O(m^{-\tfrac32}),\\
\operatorname{Bias}(\widehat Z)
&=-\frac1m\,U\Gamma V^{\!\top}+O(m^{-\tfrac32}),\\
\operatorname{Var}(\widehat Z)
&=\frac1{nm}\,{\cal J}\Sigma{\cal J}^{\!\top}
  +O\bigl((nm)^{-\tfrac32}\bigr).
\end{aligned}$$

---

## **Claim 5 – Double orthogonalisation keeps \(O(1)\) bias**

**Why you care**  
If your pipeline is “orthogonalise → all-reduce → orthogonalise again”
you still pay the bias; only its sign flips in the orthogonal subspace.

**Justification**  
Jacobian maps the constant bias block \(B\) to \(+B_\perp\); quadratic
terms add \(-B/n\).  Net expectation:
$$\mathbb E[\operatorname{Ortho}(\bar W)]
  = UV^{\!\top}+B_\perp-\tfrac1n B+O(\Sigma^{2}).$$
The \(B_\perp\) term survives for all \(n\).

---

## **Claim 6 – One scalar norm per sample restores \(O(1/n)\) bias**

**Why you care**  
Cheap fix: stream \(\rho_x=\lVert g_x\rVert_F\) (one float) together with
\(W_x\).  Communication rises minimally yet consistency is restored.

**Justification**  
Because \(g_x=\rho_x W_x\):
$$\mathbb E[\rho_x W_x]=\mathbb E[g_x]=G.$$
Average then a single \(\operatorname{Ortho}\) → Claim 1 applies, bias
\(\propto1/n\).

---

## **Claim 7 – Richardson extrapolation isolates bias coefficient**

**Why you care**  
No giant reference batch needed.  Two offline passes (size \(m\) and
synthetic \(2m\)) let you predict how bias shrinks if you doubled the
all-gather.

**Justification**  
Assume  
\(\mathbb E[Z_{(b)}]=Z_\infty+\dfrac{c}{b}+O(b^{-2})\).
Empirical means \(\bar Z_{(m)},\bar Z_{(2m)}\):

$$\begin{aligned}
Z_{\text{Rich}} &= 2\bar Z_{(2m)}-\bar Z_{(m)},\\
\mathbb E[Z_{\text{Rich}}] &= Z_\infty+O(m^{-2}),\\
\widehat c &= 2m(\bar Z_{(m)}-\bar Z_{(2m)})=c+O(m^{-1}).
\end{aligned}$$

---

## **Claim 8 – Practical bias-vs-variance diagnostic**

**Why you care**  
Tells you—*with gradients you already computed*—whether to spend on
bandwidth (to kill bias) or on model-soup / EMA (to kill variance).

**Justification**  
1. Raw micro-batch gradients \(G_i\) → cheap updates \(Z_i\).  
2. Variance  
   $$\sigma_{\text{var}}^{2}
     =\frac1{K-1}\sum_i\lVert Z_i-\bar Z_{(m)}\rVert_F^{2}.$$  
3. Synthetic \(2m\) batches  
   \(\widetilde G_j=\tfrac12(G_{2j-1}+G_{2j})\) → \(Z_j^{(2m)}\).  
4. Bias magnitude  
   $$\text{bias}_{(m)}\approx
     \lVert\bar Z_{(m)}-\bar Z_{(2m)}\rVert_F.$$  
5. Signal-to-noise ratio  
   $$\text{SNR}=\dfrac{\text{bias}_{(m)}}{\sigma_{\text{var}}}.$$

Low SNR ⇒ variance dominates (model-soup).  
High SNR ⇒ bias dominates (larger all-gathers).

---