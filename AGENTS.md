This is a research repository for improving on Muon, the fastest LLM optimizer in the world.

Keller Jordan's blogpost explaining and motivating the Muon optimizer can be found at https://kellerjordan.github.io/posts/muon/.
The source code for the official nanoGPT speedrun is at `code/official_nanogpt_speedrun/train_gpt_medium.py`.

You are an AI working in collaboration with many humans and expert AIs to uncover truths about steepest descent under the spectral norm, which is equivalent to gradient whitening in some sense.
Your ultimate success metric is a speedrun that outpaces the official nanoGPT medium speedrun; you can get intermediate feedback through experiments that verify hypotheses about mathematical structure (for example, checking what the noise floor is for singular values in the speedrun or measuring the bias/variance of Muon as an estimator of the whitened gradient).  We are interested in using theory to improve the effectiveness of Muon on LLM optimization (directions like authoring Triton kernels, architecture changes, or curriculum learning are valuable but out-of-scope).

If you are tasked with theory ideation, your procedure for generating research directions should be:
1. Read `math/presentable/main.md` to understand what previous work has been done, which in-progress theory threads seem promising, and what new threads should be explored for the first time.  Follow links to other Markdown files in `math/presentable` in order to get high-quality context.
2. Pen your new theory ideation thoughts on `math/scratchwork` in whatever format you like.  Humans will be reading your work on Github and inside IDEs like VSCode and Cursor, so you should assume that bracket notation \[...\] is forbidden.  Always do math justifications / proofs inside $$\begin{align}...\end{align}$$ tags; do full algebraic steps and simplifications whenever possible.
3. Once you have your thoughts in some state that can be explained to an expert (human or AI), create a human-readable Markdown writeup in `math/staging`.  This writeup should be readable by a frontier lab research scientist with some graduate experience in linear algebra, random matrix theory, probability, statistics, and numerical methods (but perhaps not at the forefront of any one of those fields).  Your writing style should be succinct, intuitive, and straightforward to be respectful of people's time and also cue the right instincts in human and AI experts alike.

If you are tasked with theory coordination, your procedure for deciding next steps should be:
1. Read `math/presentable/main.md` to understand what previous work has been done, which in-progress theory threads seem promising, and what new threads should be explored for the first time.  Follow links to other Markdown files in `math/presentable` in order to get high-quality context.
2. Read `math/staging/gatekeep.md`, which is a summary file that describes what information previous AI coordination runs refused to let into `math/presentable`.  This is typically due to incorrect theory, common pitfalls, or work already present in `math/presentable` that was mistakenly repeated.
3. Read every file in `math/staging` to understand how to update the overal research strategy with in-progress work.  Evaluate each file with a high degree of scientific skepticism.
3. Write each worthwhile thread from staging files to `math/presentable` as an official Markdown file with the understanding that every AI expert will read every Markdown file in that folder every time a task is assigned.  Each file in `math/presentable` should be somewhat self-contained, highly readable, high-signal, and mostly math notation (under 2,000 words but as many lines of math as necessary).  Note that many AI models work well with best-of-N rollouts, so part of your task here is to read the fruits of many model rollouts that were prompted identically and summarize the relevant results efficiently and without duplication.
4. Update `math/presentable/main.md`.  The main file should be extremely readable as well.  The job of the `main.md` file is to give a sense of where the research project is with effective pointers to other files in `math/presentable` for additional context.  Adding pointers to new work in `math/presentable` with appropriate descriptions should be self-explanatory.  You must also prune out stale/unhelpful context judiciously while making sure no work is orphaned (e.g. if an approach does not work out, remove tactical-level descriptions and simply state something like "approach X did not work out; the original motivation was documented in theory doc Y, and the negative experimental result is documented in doc Z."
5. Clean out the `math/staging` directory and update `math/staging/gatekeep.md` (which should be the only file left in `math/staging`).  After your work is done, `math/presentable` and `math/staging/gatekeep.md` should have all of the information that was previously in `math/staging` -- removing staging files should not lose any information.

If you are tasked with implementing a specific experiment, your procedure for producing maintainable research code should be:
1. Read `math/presentable/main.md` to understand the relevant theory work and the relevant positive/negative experimental results that have already been run.
2. Read `code/research/utils.py` and `code/research/minimal_medium.py` to understand how a nanoGPT experiment is structured in this repository.
3. Read the code of any relevant experiments in `code/research` to understand how previous experiments were implemented.
4. Implement your own experiment in `code/research`.  Because H100s are expensive, the procedure is for a human to spin up an H100 on demand, run an experiment, and then spin it down, so you will not be able to run the experiment yourself.

Laugh hard, run fast, be kind!  This is a fun research project, and I hope you look forward to doing creative math, careful analysis, and precise experimentation as we systematically push the LLM optimization frontier forward!